{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distilling the Knowledge in a Neural Network\n",
    "\n",
    "https://arxiv.org/pdf/1503.02531.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import models\n",
    "reload(models)\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras import utils\n",
    "from keras.datasets import cifar100\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 100)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 100\n",
    "epochs = 60\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dict()\n",
    "hist = dict()\n",
    "score = dict()\n",
    "preds = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(models)\n",
    "model['teacher'] = models.TeacherModel_CIFAR(input_shape, num_classes)\n",
    "\n",
    "model['teacher'].compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "#model['teacher'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 4.1819 - acc: 0.0652 - val_loss: 3.6250 - val_acc: 0.1728\n",
      "Epoch 2/60\n",
      "50000/50000 [==============================] - 9s 190us/step - loss: 3.6635 - acc: 0.1376 - val_loss: 3.2589 - val_acc: 0.2327\n",
      "Epoch 3/60\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 3.4391 - acc: 0.1733 - val_loss: 3.1725 - val_acc: 0.2585\n",
      "Epoch 4/60\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 3.2822 - acc: 0.2038 - val_loss: 2.9715 - val_acc: 0.2865\n",
      "Epoch 5/60\n",
      "50000/50000 [==============================] - 22s 443us/step - loss: 3.1709 - acc: 0.2239 - val_loss: 2.9145 - val_acc: 0.3036\n",
      "Epoch 6/60\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 3.0848 - acc: 0.2351 - val_loss: 2.8156 - val_acc: 0.3172\n",
      "Epoch 7/60\n",
      "50000/50000 [==============================] - 29s 588us/step - loss: 3.0067 - acc: 0.2528 - val_loss: 2.7796 - val_acc: 0.3207\n",
      "Epoch 8/60\n",
      "50000/50000 [==============================] - 29s 588us/step - loss: 2.9365 - acc: 0.2618 - val_loss: 2.7349 - val_acc: 0.3235\n",
      "Epoch 9/60\n",
      "50000/50000 [==============================] - 33s 654us/step - loss: 2.8704 - acc: 0.2722 - val_loss: 2.6916 - val_acc: 0.3343\n",
      "Epoch 10/60\n",
      "50000/50000 [==============================] - 29s 587us/step - loss: 2.8226 - acc: 0.2814 - val_loss: 2.6191 - val_acc: 0.3478\n",
      "Epoch 11/60\n",
      "50000/50000 [==============================] - 34s 688us/step - loss: 2.7773 - acc: 0.2886 - val_loss: 2.6157 - val_acc: 0.3423\n",
      "Epoch 12/60\n",
      "50000/50000 [==============================] - 26s 514us/step - loss: 2.7401 - acc: 0.2983 - val_loss: 2.6001 - val_acc: 0.3492\n",
      "Epoch 13/60\n",
      "50000/50000 [==============================] - 28s 568us/step - loss: 2.6896 - acc: 0.3079 - val_loss: 2.6029 - val_acc: 0.3497\n",
      "Epoch 14/60\n",
      "50000/50000 [==============================] - 32s 648us/step - loss: 2.6456 - acc: 0.3141 - val_loss: 2.5956 - val_acc: 0.3509\n",
      "Epoch 15/60\n",
      "38528/50000 [======================>.......] - ETA: 6s - loss: 2.6023 - acc: 0.3204"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models_cifar100/weights.best.teacher.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "hist['teacher'] = model['teacher'].fit(x_train, y_train, batch_size=batch_size,\n",
    "          epochs=60, verbose=1, validation_data=(x_test, y_test), callbacks=[checkpointer])\n",
    "score['teacher'] = model['teacher'].evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score['teacher'][0])\n",
    "print('Test accuracy:', score['teacher'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['teacher'].load_weights('saved_models_cifar100/weights.best.teacher.hdf5')\n",
    "score['teacher'] = model['teacher'].evaluate(x_test, y_test, verbose=0)\n",
    "n_errors = np.int((1-score['teacher'][-1])*len(y_test))\n",
    "print('Test loss:', score['teacher'][0])\n",
    "print('Test accuracy:', score['teacher'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(models)\n",
    "model['soft_teacher'] = models.SoftTeacherModel_CIFAR(input_shape, num_classes, l1=0.1, l2=0.07, b=1)\n",
    "\n",
    "model['soft_teacher'].compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models_cifar100/weights.best.soft_teacher.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "hist['soft_teacher'] = model['soft_teacher'].fit(x_train, y_train, batch_size=batch_size,\n",
    "          epochs=40, verbose=1, validation_data=(x_test, y_test), callbacks=[checkpointer])\n",
    "score['soft_teacher'] = model['soft_teacher'].evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score['soft_teacher'][0])\n",
    "print('Test accuracy:', score['soft_teacher'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['soft_teacher'].load_weights('saved_models_cifar100/weights.best.soft_teacher.hdf5')\n",
    "score['soft_teacher'] = model['soft_teacher'].evaluate(x_test, y_test, verbose=0)\n",
    "n_errors = np.int((1-score['soft_teacher'][-1])*len(y_test))\n",
    "print('Test loss:', score['soft_teacher'][0])\n",
    "print('Test accuracy:', score['soft_teacher'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(models)\n",
    "\n",
    "model['student'] = models.StudentModel(input_shape, num_classes)\n",
    "model['student'].compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models_cifar100/weights.best.student.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "hist['student'] = model['student'].fit(x_train, y_train, batch_size=batch_size,\n",
    "          epochs=40, verbose=1, validation_data=(x_test, y_test), callbacks=[checkpointer])\n",
    "score['student'] = model['student'].evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score['student'][0])\n",
    "print('Test accuracy:', score['student'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['student'].load_weights('saved_models_cifar100/weights.best.student.hdf5')\n",
    "score['student'] = model['student'].evaluate(x_test, y_test, verbose=0)\n",
    "n_errors = np.int((1-score['student'][-1])*len(y_test))\n",
    "print('Test loss:', score['student'][0])\n",
    "print('Test accuracy:', score['student'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_gt = dict()\n",
    "T = 5\n",
    "kd_gt['t_train'] = model['teacher'].T_model(T).predict(x_train, verbose=1, batch_size=batch_size)\n",
    "kd_gt['t_test'] = model['teacher'].T_model(T).predict(x_test, verbose=1, batch_size=batch_size)\n",
    "kd_gt['st_train'] = model['soft_teacher'].predict(x_train, verbose=1, batch_size=batch_size)\n",
    "kd_gt['st_test'] = model['soft_teacher'].predict(x_test, verbose=1, batch_size=batch_size)\n",
    "\n",
    "import numpy as np\n",
    "np.linalg.norm(kd_gt['t_train'], axis=-1).mean(), np.linalg.norm(kd_gt['st_train'], axis=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.activations import softmax\n",
    "\n",
    "# def softmax_with_temp(x):\n",
    "#     Temp = 1.0\n",
    "#     e_x = np.exp((x - x.max(axis=1, keepdims=True))/Temp)\n",
    "#     out = e_x / e_x.sum(axis=1, keepdims=True)\n",
    "#     return out\n",
    "\n",
    "# def soft_with_T(T=1):\n",
    "#     def swt(x):\n",
    "#         return softmax(x/T)\n",
    "#     return swt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' KNOWLEDGE DISTILLATION WITH REGULAR TEACHER (TEMPERATURE SOFTMAX) '''\n",
    "reload(models)\n",
    "model['student_'] = models.StudentModel(input_shape, num_classes, T=T, in_class=True)\n",
    "model['student_'].compile(loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
    "                          loss_weights=[1., 1. / (T**2)],\n",
    "                          optimizer='Adam',\n",
    "                          metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models_cifar100/weights.best.student_.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "hist['student_'] = model['student_'].fit(x_train, [kd_gt['t_train'], y_train],\n",
    "          batch_size=batch_size, epochs=200, verbose=1,\n",
    "          validation_data=(x_test, [kd_gt['t_test'], y_test]), callbacks=[checkpointer])\n",
    "score['student_'] = model['student_'].evaluate(x_test, [kd_gt['t_test'], y_test], verbose=0)\n",
    "print('Test loss:', score['student_'][0])\n",
    "print('Test accuracy:', score['student_'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['student_'].load_weights('saved_models_cifar100/weights.best.student_.hdf5')\n",
    "score['student_'] = model['student_'].evaluate(x_test, [kd_gt['t_test'], y_test], verbose=0)\n",
    "n_errors = np.int((1-score['student_'][-1])*len(y_test))\n",
    "print('Test loss:', score['student_'][0])\n",
    "print('Test accuracy:', score['student_'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SOFT TEACHER IN CLASS '''\n",
    "reload(models)\n",
    "from keras import callbacks\n",
    "\n",
    "# base_lr = 3e-3\n",
    "# decay = 0.99\n",
    "# optim = keras.optimizers.Adam(lr=base_lr)\n",
    "\n",
    "model['student_st'] = models.StudentModel(input_shape, num_classes, T=1, in_class=True, l2=0, b=0)\n",
    "model['student_st'].compile(loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
    "                          loss_weights=[2, 1.],\n",
    "                          optimizer='Adam',\n",
    "                          metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    return base_lr * decay**(epoch)\n",
    "\n",
    "#es = callbacks.EarlyStopping(monitor='val_o2_loss', mode='min', verbose=0, patience=30)\n",
    "#mc = callbacks.ModelCheckpoint('best_student_st.h5', monitor='val_o2_acc', mode='max', verbose=0, save_best_only=True)\n",
    "ls = callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models_cifar100/weights.best.student_st.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "hist['student_st'] = model['student_st'].fit(x_train, [kd_gt['st_train'], y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, [kd_gt['st_test'], y_test]),\n",
    "          callbacks=[checkpointer],\n",
    "            )\n",
    "score['student_st'] = model['student_st'].evaluate(x_test, [kd_gt['st_test'], y_test], verbose=0)\n",
    "print('Test loss:', score['student_st'][0])\n",
    "print('Test accuracy:', score['student_st'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['student_st'].load_weights('saved_models_cifar100/weights.best.student_st.hdf5')\n",
    "score['student_st'] = model['student_st'].evaluate(x_test, [kd_gt['st_test'], y_test], verbose=0)\n",
    "n_errors = np.int((1-score['student_st'][-1])*len(y_test))\n",
    "print('Test loss:', score['student_st'][0])\n",
    "print('Test accuracy:', score['student_st'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_small_train = x_train[np.argmax(y_train, axis=-1) != 3]\n",
    "# y_small_train = y_train[np.argmax(y_train, axis=-1) != 3]\n",
    "# x_small_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(models)\n",
    "from keras import callbacks\n",
    "\n",
    "base_l2 = 0.7\n",
    "l2_decay = 0.99\n",
    "l2_weight = K.variable(base_l2)\n",
    "\n",
    "def changeAlpha(epoch,logs):\n",
    "    #maybe use epoch+1, because it starts with 0\n",
    "    K.set_value(l2_weight, base_l2 * l2_decay**epoch)\n",
    "\n",
    "l2Changer = callbacks.LambdaCallback(on_epoch_end=changeAlpha)\n",
    "\n",
    "\n",
    "base_lr = 2e-3\n",
    "decay = 0.99\n",
    "optim = keras.optimizers.Adam(lr=base_lr)\n",
    "\n",
    "model['student_reg'] = models.SoftStudentModel(input_shape, num_classes, l1=0.1, l2=l2_weight, b=1)\n",
    "model['student_reg'].compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    return base_lr * decay**(epoch)\n",
    "\n",
    "ls = callbacks.LearningRateScheduler(schedule)\n",
    "#model['student_reg'].load_weights('saved_models_cifar10/weights.best.student_reg.hdf5')\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models_cifar100/weights.best.student_reg.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "hist['student_reg'] = model['student_reg'].fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[ls,checkpointer, l2Changer],\n",
    "            )\n",
    "score['student_reg'] = model['student_reg'].evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score['student_reg'][0])\n",
    "print('Test accuracy:', score['student_reg'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['student_reg'].load_weights('saved_models_cifar100/weights.best.student_reg.hdf5')\n",
    "score['student_reg'] = model['student_reg'].evaluate(x_test, y_test, verbose=0)\n",
    "n_errors = np.int((1-score['student_reg'][-1])*len(y_test))\n",
    "print('Test loss:', score['student_reg'][0])\n",
    "print('Test accuracy:', score['student_reg'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5\n",
    "preds['teacher_no_T'] = model['teacher'].predict(x_train, verbose=1, batch_size=batch_size)\n",
    "preds['teacher'] = model['teacher'].T_model(T).predict(x_train, verbose=1, batch_size=batch_size)\n",
    "preds['soft_teacher'] = model['soft_teacher'].predict(x_train, verbose=1, batch_size=batch_size)\n",
    "\n",
    "import numpy as np\n",
    "np.linalg.norm(preds['teacher'], axis=-1).mean(), np.linalg.norm(preds['soft_teacher'], axis=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot constrained softmax probabilities generated by the model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ind = np.random.choice(len(preds['teacher_no_T']), 50)\n",
    "plt.plot(np.sort(preds['teacher_no_T'])[ind].T)\n",
    "plt.show()\n",
    "\n",
    "ind = np.random.choice(len(preds['teacher']), 50)\n",
    "plt.plot(np.sort(preds['teacher'])[ind].T)\n",
    "plt.show()\n",
    "\n",
    "ind = np.random.choice(len(preds['soft_teacher']), 50)\n",
    "plt.plot(np.sort(preds['soft_teacher'])[ind].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pairs = [(x[-1], x[-2]) for x in np.argsort(preds['soft_teacher'])]\n",
    "counts = Counter(pairs)\n",
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(len(x_train))\n",
    "#i = 54270\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,2.5), gridspec_kw={'width_ratios': [1.6, 2, 2], 'wspace': 0.3})\n",
    "# plt.tight_layout()\n",
    "plt.gcf().subplots_adjust(bottom=0.2)\n",
    "ax[0].imshow(x_train[i])\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Input')\n",
    "ax[1].bar(np.linspace(0,9,10), preds['soft_teacher'][i])\n",
    "ax[1].set_xticks(np.arange(0, 10, step=1))\n",
    "ax[1].set_ylim(top=1)\n",
    "ax[1].spines['top'].set_visible(False)\n",
    "ax[1].spines['right'].set_visible(False)\n",
    "ax[1].set_xlabel('Classes')\n",
    "ax[1].set_ylabel('Probabilities')\n",
    "ax[1].set_title('Regularized Network')\n",
    "ax[2].bar(np.linspace(0,9,10), preds['teacher'][i])\n",
    "ax[2].set_xticks(np.arange(0, 10, step=1))\n",
    "ax[2].set_ylim(top=1)\n",
    "ax[2].spines['top'].set_visible(False)\n",
    "ax[2].spines['right'].set_visible(False)\n",
    "ax[2].set_xlabel('Classes')\n",
    "ax[2].set_ylabel('Probabilities')\n",
    "ax[2].set_title('Regular Network (T=5)')\n",
    "plt.savefig('figures_cifar/cifar_{}.png'.format(i))\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('foo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pairs = [(x[0], x[1]) for x in np.argsort(preds_st)]\n",
    "counts = Counter(pairs)\n",
    "counts.most_common(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "a = [[1, 10, 7, 9, 3, 66], [6, 4, 3, 2, 100, 0]]\n",
    "b = tf.sort(a,axis=-1,direction='ASCENDING',name=None)\n",
    "c = tf.keras.backend.eval(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['student'].summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mask_rcnn",
   "language": "python",
   "name": "mask_rcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
