{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distilling the Knowledge in a Neural Network\n",
    "\n",
    "https://arxiv.org/pdf/1503.02531.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import models\n",
    "reload(models)\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1280\n",
    "num_classes = 10\n",
    "epochs = 60\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dict()\n",
    "hist = dict()\n",
    "score = dict()\n",
    "preds = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 09:25:39.304006 139704598443840 deprecation_wrapper.py:119] From /home/acar/.virtualenvs/dllab/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(models)\n",
    "model['teacher'] = models.TeacherModel(input_shape, num_classes)\n",
    "\n",
    "model['teacher'].compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.6150 - acc: 0.8142 - val_loss: 0.1535 - val_acc: 0.9553\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.1704 - acc: 0.9504 - val_loss: 0.0718 - val_acc: 0.9775\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.1046 - acc: 0.9690 - val_loss: 0.0522 - val_acc: 0.9835\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0816 - acc: 0.9760 - val_loss: 0.0442 - val_acc: 0.9856\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0686 - acc: 0.9794 - val_loss: 0.0366 - val_acc: 0.9865\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0578 - acc: 0.9826 - val_loss: 0.0360 - val_acc: 0.9879\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0521 - acc: 0.9843 - val_loss: 0.0341 - val_acc: 0.9879\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0479 - acc: 0.9853 - val_loss: 0.0315 - val_acc: 0.9893\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0438 - acc: 0.9866 - val_loss: 0.0279 - val_acc: 0.9905\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0387 - acc: 0.9880 - val_loss: 0.0299 - val_acc: 0.9894\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0356 - acc: 0.9887 - val_loss: 0.0278 - val_acc: 0.9902\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0320 - acc: 0.9900 - val_loss: 0.0280 - val_acc: 0.9911\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0320 - acc: 0.9899 - val_loss: 0.0270 - val_acc: 0.9903\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0288 - acc: 0.9911 - val_loss: 0.0278 - val_acc: 0.9896\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0281 - acc: 0.9909 - val_loss: 0.0273 - val_acc: 0.9912\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0269 - acc: 0.9914 - val_loss: 0.0266 - val_acc: 0.9914\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0239 - acc: 0.9922 - val_loss: 0.0292 - val_acc: 0.9911\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0228 - acc: 0.9925 - val_loss: 0.0276 - val_acc: 0.9903\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0214 - acc: 0.9933 - val_loss: 0.0255 - val_acc: 0.9914\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.0202 - acc: 0.9933 - val_loss: 0.0263 - val_acc: 0.9917\n",
      "Test loss: 0.0262781987335713\n",
      "Test accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "hist['teacher'] = model['teacher'].fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score['teacher'] = model['teacher'].evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score['teacher'][0])\n",
    "print('Test accuracy:', score['teacher'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0262781987335713\n",
      "Test accuracy: 0.9917\n",
      "Test errors: 82\n"
     ]
    }
   ],
   "source": [
    "score['teacher'] = model['teacher'].evaluate(x_test, y_test, verbose=0)\n",
    "n_errors = np.int((1-score['teacher'][-1])*len(y_test))\n",
    "print('Test loss:', score['teacher'][0])\n",
    "print('Test accuracy:', score['teacher'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(models)\n",
    "model['soft_teacher'] = models.SoftTeacherModel(input_shape, num_classes, l1=0.1, l2=0.05, b=1.7)\n",
    "\n",
    "model['soft_teacher'].compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 1.4975 - acc: 0.7563 - val_loss: 1.1672 - val_acc: 0.9550\n",
      "Epoch 2/60\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 1.2064 - acc: 0.9439 - val_loss: 1.1130 - val_acc: 0.9780\n",
      "Epoch 3/60\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 1.1680 - acc: 0.9638 - val_loss: 1.1015 - val_acc: 0.9809\n",
      "Epoch 4/60\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 1.1506 - acc: 0.9717 - val_loss: 1.0937 - val_acc: 0.9832\n",
      "Epoch 5/60\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 1.1388 - acc: 0.9766 - val_loss: 1.0897 - val_acc: 0.9843\n",
      "Epoch 6/60\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 1.1312 - acc: 0.9793 - val_loss: 1.0856 - val_acc: 0.9852\n",
      "Epoch 7/60\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 1.1239 - acc: 0.9823 - val_loss: 1.0835 - val_acc: 0.9863\n",
      "Epoch 8/60\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 1.1199 - acc: 0.9833 - val_loss: 1.0793 - val_acc: 0.9871\n",
      "Epoch 9/60\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 1.1151 - acc: 0.9851 - val_loss: 1.0801 - val_acc: 0.9873\n",
      "Epoch 10/60\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 1.1121 - acc: 0.9865 - val_loss: 1.0781 - val_acc: 0.9887\n",
      "Epoch 11/60\n",
      "60000/60000 [==============================] - 12s 207us/step - loss: 1.1100 - acc: 0.9872 - val_loss: 1.0762 - val_acc: 0.9893\n",
      "Epoch 12/60\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 1.1070 - acc: 0.9877 - val_loss: 1.0765 - val_acc: 0.9889\n",
      "Epoch 13/60\n",
      "60000/60000 [==============================] - 12s 206us/step - loss: 1.1045 - acc: 0.9894 - val_loss: 1.0746 - val_acc: 0.9894\n",
      "Epoch 14/60\n",
      "60000/60000 [==============================] - 12s 206us/step - loss: 1.1027 - acc: 0.9896 - val_loss: 1.0745 - val_acc: 0.9896\n",
      "Epoch 15/60\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 1.1012 - acc: 0.9898 - val_loss: 1.0747 - val_acc: 0.9896\n",
      "Epoch 16/60\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 1.0994 - acc: 0.9912 - val_loss: 1.0750 - val_acc: 0.9911\n",
      "Epoch 17/60\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 1.0979 - acc: 0.9911 - val_loss: 1.0732 - val_acc: 0.9903\n",
      "Epoch 18/60\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 1.0967 - acc: 0.9918 - val_loss: 1.0715 - val_acc: 0.9908\n",
      "Epoch 19/60\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 1.0951 - acc: 0.9922 - val_loss: 1.0720 - val_acc: 0.9913\n",
      "Epoch 20/60\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 1.0938 - acc: 0.9930 - val_loss: 1.0708 - val_acc: 0.9917\n",
      "Epoch 21/60\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 1.0924 - acc: 0.9927 - val_loss: 1.0715 - val_acc: 0.9912\n",
      "Epoch 22/60\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 1.0918 - acc: 0.9933 - val_loss: 1.0728 - val_acc: 0.9905\n",
      "Epoch 23/60\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 1.0906 - acc: 0.9942 - val_loss: 1.0699 - val_acc: 0.9916\n",
      "Epoch 24/60\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 1.0898 - acc: 0.9938 - val_loss: 1.0707 - val_acc: 0.9917\n",
      "Epoch 25/60\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 1.0887 - acc: 0.9946 - val_loss: 1.0690 - val_acc: 0.9921\n",
      "Epoch 26/60\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 1.0873 - acc: 0.9951 - val_loss: 1.0707 - val_acc: 0.9911\n",
      "Epoch 27/60\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 1.0868 - acc: 0.9952 - val_loss: 1.0695 - val_acc: 0.9914\n",
      "Epoch 28/60\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 1.0859 - acc: 0.9953 - val_loss: 1.0689 - val_acc: 0.9917\n",
      "Epoch 29/60\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 1.0853 - acc: 0.9951 - val_loss: 1.0694 - val_acc: 0.9916\n",
      "Epoch 30/60\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 1.0847 - acc: 0.9954 - val_loss: 1.0701 - val_acc: 0.9916\n",
      "Epoch 31/60\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 1.0840 - acc: 0.9957 - val_loss: 1.0690 - val_acc: 0.9920\n",
      "Epoch 32/60\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 1.0833 - acc: 0.9958 - val_loss: 1.0695 - val_acc: 0.9925\n",
      "Epoch 33/60\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 1.0824 - acc: 0.9964 - val_loss: 1.0686 - val_acc: 0.9912\n",
      "Epoch 34/60\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 1.0819 - acc: 0.9963 - val_loss: 1.0681 - val_acc: 0.9918\n",
      "Epoch 35/60\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 1.0816 - acc: 0.9965 - val_loss: 1.0685 - val_acc: 0.9921\n",
      "Epoch 36/60\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 1.0809 - acc: 0.9965 - val_loss: 1.0678 - val_acc: 0.9918\n",
      "Epoch 37/60\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 1.0804 - acc: 0.9967 - val_loss: 1.0686 - val_acc: 0.9925\n",
      "Epoch 38/60\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 1.0802 - acc: 0.9964 - val_loss: 1.0673 - val_acc: 0.9922\n",
      "Epoch 39/60\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 1.0791 - acc: 0.9971 - val_loss: 1.0675 - val_acc: 0.9918\n",
      "Epoch 40/60\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 1.0788 - acc: 0.9969 - val_loss: 1.0678 - val_acc: 0.9927\n",
      "Epoch 41/60\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 1.0785 - acc: 0.9971 - val_loss: 1.0668 - val_acc: 0.9924\n",
      "Epoch 42/60\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 1.0782 - acc: 0.9972 - val_loss: 1.0684 - val_acc: 0.9920\n",
      "Epoch 43/60\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 1.0776 - acc: 0.9975 - val_loss: 1.0675 - val_acc: 0.9922\n",
      "Epoch 44/60\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 1.0770 - acc: 0.9976 - val_loss: 1.0680 - val_acc: 0.9927\n",
      "Epoch 45/60\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 1.0768 - acc: 0.9975 - val_loss: 1.0672 - val_acc: 0.9924\n",
      "Epoch 46/60\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 1.0761 - acc: 0.9979 - val_loss: 1.0678 - val_acc: 0.9919\n",
      "Epoch 47/60\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 1.0760 - acc: 0.9978 - val_loss: 1.0669 - val_acc: 0.9925\n",
      "Epoch 48/60\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 1.0757 - acc: 0.9979 - val_loss: 1.0667 - val_acc: 0.9925\n",
      "Epoch 49/60\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 1.0753 - acc: 0.9979 - val_loss: 1.0663 - val_acc: 0.9924\n",
      "Epoch 50/60\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 1.0752 - acc: 0.9980 - val_loss: 1.0667 - val_acc: 0.9925\n",
      "Epoch 51/60\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 1.0747 - acc: 0.9981 - val_loss: 1.0664 - val_acc: 0.9927\n",
      "Epoch 52/60\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 1.0745 - acc: 0.9981 - val_loss: 1.0670 - val_acc: 0.9923\n",
      "Epoch 53/60\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 1.0742 - acc: 0.9982 - val_loss: 1.0661 - val_acc: 0.9924\n",
      "Epoch 54/60\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 1.0739 - acc: 0.9981 - val_loss: 1.0665 - val_acc: 0.9924\n",
      "Epoch 55/60\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 1.0739 - acc: 0.9982 - val_loss: 1.0671 - val_acc: 0.9922\n",
      "Epoch 56/60\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 1.0737 - acc: 0.9981 - val_loss: 1.0671 - val_acc: 0.9929\n",
      "Epoch 57/60\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 1.0735 - acc: 0.9984 - val_loss: 1.0665 - val_acc: 0.9923\n",
      "Epoch 58/60\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 1.0732 - acc: 0.9986 - val_loss: 1.0657 - val_acc: 0.9931\n",
      "Epoch 59/60\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 1.0730 - acc: 0.9983 - val_loss: 1.0670 - val_acc: 0.9921\n",
      "Epoch 60/60\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 1.0731 - acc: 0.9983 - val_loss: 1.0668 - val_acc: 0.9921\n",
      "Test loss: 0.6570694422721863\n",
      "Test accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "hist['soft_teacher'] = model['soft_teacher'].fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=60,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score['soft_teacher'] = model['soft_teacher'].evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score['soft_teacher'][0])\n",
    "print('Test accuracy:', score['soft_teacher'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.28867836813926695\n",
      "Test accuracy: 0.992\n",
      "Test errors: 80\n"
     ]
    }
   ],
   "source": [
    "score['soft_teacher'] = model['soft_teacher'].evaluate(x_test, y_test, verbose=0)\n",
    "n_errors = np.int((1-score['soft_teacher'][-1])*len(y_test))\n",
    "print('Test loss:', score['soft_teacher'][0])\n",
    "print('Test accuracy:', score['soft_teacher'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.4571 - acc: 0.8737 - val_loss: 0.1872 - val_acc: 0.9466\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.1472 - acc: 0.9576 - val_loss: 0.1209 - val_acc: 0.9634\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0965 - acc: 0.9720 - val_loss: 0.0907 - val_acc: 0.9715\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0673 - acc: 0.9803 - val_loss: 0.0764 - val_acc: 0.9779\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0475 - acc: 0.9861 - val_loss: 0.0729 - val_acc: 0.9775\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0364 - acc: 0.9899 - val_loss: 0.0650 - val_acc: 0.9808\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0270 - acc: 0.9927 - val_loss: 0.0631 - val_acc: 0.9810\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0219 - acc: 0.9942 - val_loss: 0.0604 - val_acc: 0.9812\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0143 - acc: 0.9968 - val_loss: 0.0648 - val_acc: 0.9797\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0102 - acc: 0.9981 - val_loss: 0.0650 - val_acc: 0.9811\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.0645 - val_acc: 0.9811\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0053 - acc: 0.9993 - val_loss: 0.0632 - val_acc: 0.9830\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0039 - acc: 0.9996 - val_loss: 0.0642 - val_acc: 0.9820\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0030 - acc: 0.9998 - val_loss: 0.0642 - val_acc: 0.9828\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0021 - acc: 0.9999 - val_loss: 0.0646 - val_acc: 0.9831\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0647 - val_acc: 0.9827\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0675 - val_acc: 0.9825\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 9.8742e-04 - acc: 1.0000 - val_loss: 0.0687 - val_acc: 0.9825\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 8.2917e-04 - acc: 1.0000 - val_loss: 0.0690 - val_acc: 0.9831\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 6.9813e-04 - acc: 1.0000 - val_loss: 0.0691 - val_acc: 0.9827\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 6.0782e-04 - acc: 1.0000 - val_loss: 0.0691 - val_acc: 0.9833\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 5.3772e-04 - acc: 1.0000 - val_loss: 0.0707 - val_acc: 0.9830\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 4.7045e-04 - acc: 1.0000 - val_loss: 0.0708 - val_acc: 0.9827\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 4.3078e-04 - acc: 1.0000 - val_loss: 0.0722 - val_acc: 0.9826\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 3.9235e-04 - acc: 1.0000 - val_loss: 0.0726 - val_acc: 0.9823\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 3.5801e-04 - acc: 1.0000 - val_loss: 0.0726 - val_acc: 0.9824\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 3.2399e-04 - acc: 1.0000 - val_loss: 0.0734 - val_acc: 0.9827\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 2.9042e-04 - acc: 1.0000 - val_loss: 0.0742 - val_acc: 0.9825\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 2.6663e-04 - acc: 1.0000 - val_loss: 0.0746 - val_acc: 0.9831\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 2.4867e-04 - acc: 1.0000 - val_loss: 0.0752 - val_acc: 0.9823\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 2.2591e-04 - acc: 1.0000 - val_loss: 0.0754 - val_acc: 0.9827\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 2.0870e-04 - acc: 1.0000 - val_loss: 0.0761 - val_acc: 0.9828\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 1.9403e-04 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 0.9827\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 1.8330e-04 - acc: 1.0000 - val_loss: 0.0775 - val_acc: 0.9825\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 1.7078e-04 - acc: 1.0000 - val_loss: 0.0776 - val_acc: 0.9828\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 1.5979e-04 - acc: 1.0000 - val_loss: 0.0781 - val_acc: 0.9824\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 1.4656e-04 - acc: 1.0000 - val_loss: 0.0791 - val_acc: 0.9825\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 1.3642e-04 - acc: 1.0000 - val_loss: 0.0794 - val_acc: 0.9830\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 1.2758e-04 - acc: 1.0000 - val_loss: 0.0799 - val_acc: 0.9828\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 1.1905e-04 - acc: 1.0000 - val_loss: 0.0801 - val_acc: 0.9828\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 1.1190e-04 - acc: 1.0000 - val_loss: 0.0807 - val_acc: 0.9831\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 1.0682e-04 - acc: 1.0000 - val_loss: 0.0805 - val_acc: 0.9824\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 9.8720e-05 - acc: 1.0000 - val_loss: 0.0812 - val_acc: 0.9822\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 9.3694e-05 - acc: 1.0000 - val_loss: 0.0814 - val_acc: 0.9826\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 8.7433e-05 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9826\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 8.2178e-05 - acc: 1.0000 - val_loss: 0.0824 - val_acc: 0.9828\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 7.8611e-05 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9828\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 7.3838e-05 - acc: 1.0000 - val_loss: 0.0836 - val_acc: 0.9827\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 6.9883e-05 - acc: 1.0000 - val_loss: 0.0838 - val_acc: 0.9825\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 6.6244e-05 - acc: 1.0000 - val_loss: 0.0842 - val_acc: 0.9826\n",
      "Test loss: 0.08421382986314857\n",
      "Test accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "reload(models)\n",
    "model['student'] = models.StudentModel(input_shape, num_classes)\n",
    "\n",
    "model['student'].compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist['student'] = model['student'].fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score['student'] = model['student'].evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score['student'][0])\n",
    "print('Test accuracy:', score['student'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09112229214981046\n",
      "Test accuracy: 0.9829\n",
      "Test errors: 171\n"
     ]
    }
   ],
   "source": [
    "score['student'] = model['student'].evaluate(x_test, y_test, verbose=0)\n",
    "n_errors = np.int((1-score['student'][-1])*len(y_test))\n",
    "print('Test loss:', score['student'][0])\n",
    "print('Test accuracy:', score['student'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 3s 45us/step\n",
      "10000/10000 [==============================] - 2s 179us/step\n",
      "60000/60000 [==============================] - 1s 16us/step\n",
      "10000/10000 [==============================] - 0s 17us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.36443835, 0.6188533)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kd_gt = dict()\n",
    "T = 20\n",
    "kd_gt['t_train'] = model['teacher'].T_model(T).predict(x_train, verbose=1, batch_size=batch_size)\n",
    "kd_gt['t_test'] = model['teacher'].T_model(T).predict(x_test, verbose=1, batch_size=batch_size)\n",
    "kd_gt['st_train'] = model['soft_teacher'].predict(x_train, verbose=1, batch_size=batch_size)\n",
    "kd_gt['st_test'] = model['soft_teacher'].predict(x_test, verbose=1, batch_size=batch_size)\n",
    "\n",
    "import numpy as np\n",
    "np.linalg.norm(kd_gt['t_train'], axis=-1).mean(), np.linalg.norm(kd_gt['st_train'], axis=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.activations import softmax\n",
    "\n",
    "def softmax_with_temp(x):\n",
    "    Temp = 1.0\n",
    "    e_x = np.exp((x - x.max(axis=1, keepdims=True))/Temp)\n",
    "    out = e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    return out\n",
    "\n",
    "def soft_with_T(T=1):\n",
    "    def swt(x):\n",
    "        return softmax(x/T)\n",
    "    return swt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 2.2344 - o1_loss: 2.2315 - o2_loss: 1.1668 - o1_acc: 0.7834 - o2_acc: 0.7831 - val_loss: 2.1979 - val_o1_loss: 2.1956 - val_o2_loss: 0.9103 - val_o1_acc: 0.8979 - val_o2_acc: 0.8952\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1932 - o1_loss: 2.1913 - o2_loss: 0.7798 - o1_acc: 0.9125 - o2_acc: 0.9120 - val_loss: 2.1867 - val_o1_loss: 2.1853 - val_o2_loss: 0.5904 - val_o1_acc: 0.9349 - val_o2_acc: 0.9319\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1858 - o1_loss: 2.1846 - o2_loss: 0.4879 - o1_acc: 0.9434 - o2_acc: 0.9429 - val_loss: 2.1822 - val_o1_loss: 2.1812 - val_o2_loss: 0.3990 - val_o1_acc: 0.9565 - val_o2_acc: 0.9530\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.1825 - o1_loss: 2.1816 - o2_loss: 0.3389 - o1_acc: 0.9584 - o2_acc: 0.9579 - val_loss: 2.1798 - val_o1_loss: 2.1790 - val_o2_loss: 0.3104 - val_o1_acc: 0.9662 - val_o2_acc: 0.9632\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1804 - o1_loss: 2.1798 - o2_loss: 0.2499 - o1_acc: 0.9674 - o2_acc: 0.9669 - val_loss: 2.1783 - val_o1_loss: 2.1776 - val_o2_loss: 0.2544 - val_o1_acc: 0.9721 - val_o2_acc: 0.9692\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1791 - o1_loss: 2.1786 - o2_loss: 0.1950 - o1_acc: 0.9737 - o2_acc: 0.9732 - val_loss: 2.1774 - val_o1_loss: 2.1768 - val_o2_loss: 0.2225 - val_o1_acc: 0.9749 - val_o2_acc: 0.9718\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1781 - o1_loss: 2.1777 - o2_loss: 0.1562 - o1_acc: 0.9785 - o2_acc: 0.9778 - val_loss: 2.1766 - val_o1_loss: 2.1761 - val_o2_loss: 0.1944 - val_o1_acc: 0.9780 - val_o2_acc: 0.9745\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1774 - o1_loss: 2.1771 - o2_loss: 0.1268 - o1_acc: 0.9818 - o2_acc: 0.9811 - val_loss: 2.1760 - val_o1_loss: 2.1755 - val_o2_loss: 0.1777 - val_o1_acc: 0.9803 - val_o2_acc: 0.9765\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1768 - o1_loss: 2.1765 - o2_loss: 0.1047 - o1_acc: 0.9845 - o2_acc: 0.9838 - val_loss: 2.1755 - val_o1_loss: 2.1751 - val_o2_loss: 0.1587 - val_o1_acc: 0.9828 - val_o2_acc: 0.9793\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1763 - o1_loss: 2.1761 - o2_loss: 0.0884 - o1_acc: 0.9869 - o2_acc: 0.9861 - val_loss: 2.1752 - val_o1_loss: 2.1748 - val_o2_loss: 0.1508 - val_o1_acc: 0.9836 - val_o2_acc: 0.9798\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1760 - o1_loss: 2.1758 - o2_loss: 0.0751 - o1_acc: 0.9887 - o2_acc: 0.9880 - val_loss: 2.1749 - val_o1_loss: 2.1746 - val_o2_loss: 0.1423 - val_o1_acc: 0.9842 - val_o2_acc: 0.9805\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1757 - o1_loss: 2.1755 - o2_loss: 0.0631 - o1_acc: 0.9908 - o2_acc: 0.9901 - val_loss: 2.1746 - val_o1_loss: 2.1743 - val_o2_loss: 0.1380 - val_o1_acc: 0.9854 - val_o2_acc: 0.9814\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1755 - o1_loss: 2.1753 - o2_loss: 0.0545 - o1_acc: 0.9919 - o2_acc: 0.9912 - val_loss: 2.1744 - val_o1_loss: 2.1741 - val_o2_loss: 0.1332 - val_o1_acc: 0.9855 - val_o2_acc: 0.9813\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1753 - o1_loss: 2.1752 - o2_loss: 0.0474 - o1_acc: 0.9930 - o2_acc: 0.9924 - val_loss: 2.1743 - val_o1_loss: 2.1740 - val_o2_loss: 0.1250 - val_o1_acc: 0.9861 - val_o2_acc: 0.9819\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1751 - o1_loss: 2.1750 - o2_loss: 0.0406 - o1_acc: 0.9939 - o2_acc: 0.9932 - val_loss: 2.1741 - val_o1_loss: 2.1738 - val_o2_loss: 0.1231 - val_o1_acc: 0.9874 - val_o2_acc: 0.9824\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1749 - o1_loss: 2.1749 - o2_loss: 0.0354 - o1_acc: 0.9945 - o2_acc: 0.9938 - val_loss: 2.1740 - val_o1_loss: 2.1737 - val_o2_loss: 0.1185 - val_o1_acc: 0.9873 - val_o2_acc: 0.9824\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1748 - o1_loss: 2.1747 - o2_loss: 0.0310 - o1_acc: 0.9951 - o2_acc: 0.9945 - val_loss: 2.1739 - val_o1_loss: 2.1736 - val_o2_loss: 0.1145 - val_o1_acc: 0.9879 - val_o2_acc: 0.9832\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1747 - o1_loss: 2.1746 - o2_loss: 0.0271 - o1_acc: 0.9960 - o2_acc: 0.9955 - val_loss: 2.1738 - val_o1_loss: 2.1735 - val_o2_loss: 0.1120 - val_o1_acc: 0.9894 - val_o2_acc: 0.9843\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1746 - o1_loss: 2.1745 - o2_loss: 0.0245 - o1_acc: 0.9964 - o2_acc: 0.9958 - val_loss: 2.1737 - val_o1_loss: 2.1735 - val_o2_loss: 0.1118 - val_o1_acc: 0.9892 - val_o2_acc: 0.9842\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1745 - o1_loss: 2.1745 - o2_loss: 0.0218 - o1_acc: 0.9969 - o2_acc: 0.9964 - val_loss: 2.1737 - val_o1_loss: 2.1734 - val_o2_loss: 0.1106 - val_o1_acc: 0.9894 - val_o2_acc: 0.9843\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1744 - o1_loss: 2.1744 - o2_loss: 0.0196 - o1_acc: 0.9970 - o2_acc: 0.9966 - val_loss: 2.1736 - val_o1_loss: 2.1734 - val_o2_loss: 0.1063 - val_o1_acc: 0.9893 - val_o2_acc: 0.9848\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1744 - o1_loss: 2.1743 - o2_loss: 0.0175 - o1_acc: 0.9974 - o2_acc: 0.9969 - val_loss: 2.1736 - val_o1_loss: 2.1733 - val_o2_loss: 0.1042 - val_o1_acc: 0.9896 - val_o2_acc: 0.9846\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1743 - o1_loss: 2.1743 - o2_loss: 0.0155 - o1_acc: 0.9978 - o2_acc: 0.9975 - val_loss: 2.1735 - val_o1_loss: 2.1733 - val_o2_loss: 0.1044 - val_o1_acc: 0.9903 - val_o2_acc: 0.9849\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1743 - o1_loss: 2.1742 - o2_loss: 0.0145 - o1_acc: 0.9979 - o2_acc: 0.9974 - val_loss: 2.1735 - val_o1_loss: 2.1732 - val_o2_loss: 0.1005 - val_o1_acc: 0.9902 - val_o2_acc: 0.9853\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1742 - o1_loss: 2.1742 - o2_loss: 0.0130 - o1_acc: 0.9981 - o2_acc: 0.9978 - val_loss: 2.1734 - val_o1_loss: 2.1731 - val_o2_loss: 0.0975 - val_o1_acc: 0.9910 - val_o2_acc: 0.9858\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1742 - o1_loss: 2.1741 - o2_loss: 0.0122 - o1_acc: 0.9983 - o2_acc: 0.9980 - val_loss: 2.1734 - val_o1_loss: 2.1732 - val_o2_loss: 0.0987 - val_o1_acc: 0.9905 - val_o2_acc: 0.9854\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1741 - o1_loss: 2.1741 - o2_loss: 0.0111 - o1_acc: 0.9985 - o2_acc: 0.9983 - val_loss: 2.1733 - val_o1_loss: 2.1731 - val_o2_loss: 0.0957 - val_o1_acc: 0.9912 - val_o2_acc: 0.9861\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1741 - o1_loss: 2.1740 - o2_loss: 0.0103 - o1_acc: 0.9985 - o2_acc: 0.9984 - val_loss: 2.1733 - val_o1_loss: 2.1730 - val_o2_loss: 0.0958 - val_o1_acc: 0.9907 - val_o2_acc: 0.9854\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1740 - o1_loss: 2.1740 - o2_loss: 0.0095 - o1_acc: 0.9986 - o2_acc: 0.9985 - val_loss: 2.1733 - val_o1_loss: 2.1730 - val_o2_loss: 0.0969 - val_o1_acc: 0.9913 - val_o2_acc: 0.9864\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1740 - o1_loss: 2.1740 - o2_loss: 0.0091 - o1_acc: 0.9987 - o2_acc: 0.9985 - val_loss: 2.1733 - val_o1_loss: 2.1730 - val_o2_loss: 0.0944 - val_o1_acc: 0.9917 - val_o2_acc: 0.9862\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1740 - o1_loss: 2.1739 - o2_loss: 0.0082 - o1_acc: 0.9989 - o2_acc: 0.9989 - val_loss: 2.1732 - val_o1_loss: 2.1730 - val_o2_loss: 0.0931 - val_o1_acc: 0.9915 - val_o2_acc: 0.9863\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1739 - o1_loss: 2.1739 - o2_loss: 0.0075 - o1_acc: 0.9988 - o2_acc: 0.9988 - val_loss: 2.1732 - val_o1_loss: 2.1730 - val_o2_loss: 0.0924 - val_o1_acc: 0.9912 - val_o2_acc: 0.9860\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1739 - o1_loss: 2.1739 - o2_loss: 0.0071 - o1_acc: 0.9989 - o2_acc: 0.9988 - val_loss: 2.1732 - val_o1_loss: 2.1730 - val_o2_loss: 0.0900 - val_o1_acc: 0.9916 - val_o2_acc: 0.9869\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1739 - o1_loss: 2.1739 - o2_loss: 0.0062 - o1_acc: 0.9990 - o2_acc: 0.9991 - val_loss: 2.1731 - val_o1_loss: 2.1729 - val_o2_loss: 0.0911 - val_o1_acc: 0.9914 - val_o2_acc: 0.9865\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1739 - o1_loss: 2.1738 - o2_loss: 0.0060 - o1_acc: 0.9990 - o2_acc: 0.9991 - val_loss: 2.1731 - val_o1_loss: 2.1729 - val_o2_loss: 0.0891 - val_o1_acc: 0.9918 - val_o2_acc: 0.9869\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1738 - o1_loss: 2.1738 - o2_loss: 0.0054 - o1_acc: 0.9990 - o2_acc: 0.9991 - val_loss: 2.1731 - val_o1_loss: 2.1729 - val_o2_loss: 0.0886 - val_o1_acc: 0.9918 - val_o2_acc: 0.9870\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1738 - o1_loss: 2.1738 - o2_loss: 0.0047 - o1_acc: 0.9991 - o2_acc: 0.9992 - val_loss: 2.1731 - val_o1_loss: 2.1729 - val_o2_loss: 0.0891 - val_o1_acc: 0.9922 - val_o2_acc: 0.9873\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1738 - o1_loss: 2.1738 - o2_loss: 0.0044 - o1_acc: 0.9989 - o2_acc: 0.9992 - val_loss: 2.1731 - val_o1_loss: 2.1728 - val_o2_loss: 0.0886 - val_o1_acc: 0.9923 - val_o2_acc: 0.9872\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1738 - o1_loss: 2.1737 - o2_loss: 0.0042 - o1_acc: 0.9992 - o2_acc: 0.9993 - val_loss: 2.1730 - val_o1_loss: 2.1728 - val_o2_loss: 0.0879 - val_o1_acc: 0.9921 - val_o2_acc: 0.9872\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1737 - o1_loss: 2.1737 - o2_loss: 0.0038 - o1_acc: 0.9991 - o2_acc: 0.9993 - val_loss: 2.1730 - val_o1_loss: 2.1728 - val_o2_loss: 0.0870 - val_o1_acc: 0.9921 - val_o2_acc: 0.9871\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1737 - o1_loss: 2.1737 - o2_loss: 0.0034 - o1_acc: 0.9991 - o2_acc: 0.9994 - val_loss: 2.1730 - val_o1_loss: 2.1728 - val_o2_loss: 0.0859 - val_o1_acc: 0.9918 - val_o2_acc: 0.9868\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1737 - o1_loss: 2.1737 - o2_loss: 0.0032 - o1_acc: 0.9992 - o2_acc: 0.9995 - val_loss: 2.1730 - val_o1_loss: 2.1728 - val_o2_loss: 0.0862 - val_o1_acc: 0.9924 - val_o2_acc: 0.9875\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1737 - o1_loss: 2.1737 - o2_loss: 0.0030 - o1_acc: 0.9991 - o2_acc: 0.9995 - val_loss: 2.1730 - val_o1_loss: 2.1728 - val_o2_loss: 0.0869 - val_o1_acc: 0.9919 - val_o2_acc: 0.9869\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1737 - o1_loss: 2.1737 - o2_loss: 0.0027 - o1_acc: 0.9992 - o2_acc: 0.9995 - val_loss: 2.1730 - val_o1_loss: 2.1728 - val_o2_loss: 0.0860 - val_o1_acc: 0.9923 - val_o2_acc: 0.9872\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1737 - o1_loss: 2.1736 - o2_loss: 0.0026 - o1_acc: 0.9992 - o2_acc: 0.9996 - val_loss: 2.1730 - val_o1_loss: 2.1728 - val_o2_loss: 0.0845 - val_o1_acc: 0.9921 - val_o2_acc: 0.9869\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1736 - o1_loss: 2.1736 - o2_loss: 0.0025 - o1_acc: 0.9992 - o2_acc: 0.9995 - val_loss: 2.1730 - val_o1_loss: 2.1728 - val_o2_loss: 0.0861 - val_o1_acc: 0.9919 - val_o2_acc: 0.9866\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1736 - o1_loss: 2.1736 - o2_loss: 0.0021 - o1_acc: 0.9991 - o2_acc: 0.9996 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0851 - val_o1_acc: 0.9924 - val_o2_acc: 0.9874\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1736 - o1_loss: 2.1736 - o2_loss: 0.0021 - o1_acc: 0.9992 - o2_acc: 0.9996 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0843 - val_o1_acc: 0.9924 - val_o2_acc: 0.9869\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1736 - o1_loss: 2.1736 - o2_loss: 0.0020 - o1_acc: 0.9992 - o2_acc: 0.9996 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0833 - val_o1_acc: 0.9923 - val_o2_acc: 0.9878\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1736 - o1_loss: 2.1736 - o2_loss: 0.0019 - o1_acc: 0.9990 - o2_acc: 0.9996 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0845 - val_o1_acc: 0.9920 - val_o2_acc: 0.9871\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1736 - o1_loss: 2.1736 - o2_loss: 0.0018 - o1_acc: 0.9991 - o2_acc: 0.9997 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0845 - val_o1_acc: 0.9920 - val_o2_acc: 0.9870\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1736 - o1_loss: 2.1736 - o2_loss: 0.0020 - o1_acc: 0.9991 - o2_acc: 0.9996 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0828 - val_o1_acc: 0.9923 - val_o2_acc: 0.9871\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1736 - o1_loss: 2.1736 - o2_loss: 0.0019 - o1_acc: 0.9991 - o2_acc: 0.9997 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0836 - val_o1_acc: 0.9924 - val_o2_acc: 0.9872\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0017 - o1_acc: 0.9991 - o2_acc: 0.9997 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0804 - val_o1_acc: 0.9922 - val_o2_acc: 0.9868\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0016 - o1_acc: 0.9991 - o2_acc: 0.9998 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0823 - val_o1_acc: 0.9918 - val_o2_acc: 0.9864\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0016 - o1_acc: 0.9991 - o2_acc: 0.9998 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0817 - val_o1_acc: 0.9921 - val_o2_acc: 0.9870\n",
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0015 - o1_acc: 0.9991 - o2_acc: 0.9997 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0831 - val_o1_acc: 0.9923 - val_o2_acc: 0.9869\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0014 - o1_acc: 0.9990 - o2_acc: 0.9998 - val_loss: 2.1729 - val_o1_loss: 2.1727 - val_o2_loss: 0.0839 - val_o1_acc: 0.9923 - val_o2_acc: 0.9868\n",
      "Epoch 59/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0014 - o1_acc: 0.9991 - o2_acc: 0.9998 - val_loss: 2.1729 - val_o1_loss: 2.1726 - val_o2_loss: 0.0833 - val_o1_acc: 0.9925 - val_o2_acc: 0.9871\n",
      "Epoch 60/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0013 - o1_acc: 0.9991 - o2_acc: 0.9998 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0817 - val_o1_acc: 0.9924 - val_o2_acc: 0.9870\n",
      "Epoch 61/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0012 - o1_acc: 0.9991 - o2_acc: 0.9998 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0819 - val_o1_acc: 0.9922 - val_o2_acc: 0.9866\n",
      "Epoch 62/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0013 - o1_acc: 0.9991 - o2_acc: 0.9998 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0811 - val_o1_acc: 0.9922 - val_o2_acc: 0.9868\n",
      "Epoch 63/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0013 - o1_acc: 0.9990 - o2_acc: 0.9998 - val_loss: 2.1729 - val_o1_loss: 2.1726 - val_o2_loss: 0.0831 - val_o1_acc: 0.9921 - val_o2_acc: 0.9869\n",
      "Epoch 64/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0012 - o1_acc: 0.9991 - o2_acc: 0.9998 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0807 - val_o1_acc: 0.9921 - val_o2_acc: 0.9870\n",
      "Epoch 65/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 0.0011 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0818 - val_o1_acc: 0.9921 - val_o2_acc: 0.9868\n",
      "Epoch 66/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 0.0011 - o1_acc: 0.9991 - o2_acc: 0.9998 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0814 - val_o1_acc: 0.9925 - val_o2_acc: 0.9873\n",
      "Epoch 67/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1735 - o1_loss: 2.1735 - o2_loss: 0.0012 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0813 - val_o1_acc: 0.9924 - val_o2_acc: 0.9870\n",
      "Epoch 68/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 0.0011 - o1_acc: 0.9990 - o2_acc: 0.9998 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0799 - val_o1_acc: 0.9924 - val_o2_acc: 0.9873\n",
      "Epoch 69/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 0.0011 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0814 - val_o1_acc: 0.9921 - val_o2_acc: 0.9870\n",
      "Epoch 70/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 0.0011 - o1_acc: 0.9990 - o2_acc: 0.9998 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0814 - val_o1_acc: 0.9920 - val_o2_acc: 0.9870\n",
      "Epoch 71/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 9.6184e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0798 - val_o1_acc: 0.9924 - val_o2_acc: 0.9871\n",
      "Epoch 72/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 9.9546e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0797 - val_o1_acc: 0.9924 - val_o2_acc: 0.9870\n",
      "Epoch 73/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 9.4072e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0811 - val_o1_acc: 0.9929 - val_o2_acc: 0.9875\n",
      "Epoch 74/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 8.9246e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0793 - val_o1_acc: 0.9927 - val_o2_acc: 0.9874\n",
      "Epoch 75/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 9.4674e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0790 - val_o1_acc: 0.9921 - val_o2_acc: 0.9873\n",
      "Epoch 76/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 9.4723e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0796 - val_o1_acc: 0.9924 - val_o2_acc: 0.9872\n",
      "Epoch 77/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 9.2239e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0806 - val_o1_acc: 0.9929 - val_o2_acc: 0.9874\n",
      "Epoch 78/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 9.3235e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0790 - val_o1_acc: 0.9925 - val_o2_acc: 0.9873\n",
      "Epoch 79/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 8.8289e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0796 - val_o1_acc: 0.9920 - val_o2_acc: 0.9871\n",
      "Epoch 80/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1734 - o1_loss: 2.1734 - o2_loss: 9.3202e-04 - o1_acc: 0.9990 - o2_acc: 0.9998 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0793 - val_o1_acc: 0.9924 - val_o2_acc: 0.9873\n",
      "Epoch 81/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.6898e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0788 - val_o1_acc: 0.9925 - val_o2_acc: 0.9871\n",
      "Epoch 82/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.2011e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0809 - val_o1_acc: 0.9923 - val_o2_acc: 0.9871\n",
      "Epoch 83/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.4072e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0781 - val_o1_acc: 0.9919 - val_o2_acc: 0.9871\n",
      "Epoch 84/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.2162e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0792 - val_o1_acc: 0.9923 - val_o2_acc: 0.9871\n",
      "Epoch 85/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.2388e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0791 - val_o1_acc: 0.9919 - val_o2_acc: 0.9867\n",
      "Epoch 86/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.5352e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0799 - val_o1_acc: 0.9923 - val_o2_acc: 0.9871\n",
      "Epoch 87/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.8203e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0791 - val_o1_acc: 0.9926 - val_o2_acc: 0.9875\n",
      "Epoch 88/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.0052e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1728 - val_o1_loss: 2.1726 - val_o2_loss: 0.0798 - val_o1_acc: 0.9921 - val_o2_acc: 0.9870\n",
      "Epoch 89/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.1203e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0795 - val_o1_acc: 0.9926 - val_o2_acc: 0.9876\n",
      "Epoch 90/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.8607e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0792 - val_o1_acc: 0.9926 - val_o2_acc: 0.9874\n",
      "Epoch 91/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.3792e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0783 - val_o1_acc: 0.9923 - val_o2_acc: 0.9872\n",
      "Epoch 92/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.1889e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0783 - val_o1_acc: 0.9923 - val_o2_acc: 0.9873\n",
      "Epoch 93/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.3849e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0789 - val_o1_acc: 0.9922 - val_o2_acc: 0.9871\n",
      "Epoch 94/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 8.0003e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0774 - val_o1_acc: 0.9928 - val_o2_acc: 0.9875\n",
      "Epoch 95/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.5535e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0774 - val_o1_acc: 0.9927 - val_o2_acc: 0.9876\n",
      "Epoch 96/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.4374e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0780 - val_o1_acc: 0.9926 - val_o2_acc: 0.9873\n",
      "Epoch 97/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.1796e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0796 - val_o1_acc: 0.9922 - val_o2_acc: 0.9873\n",
      "Epoch 98/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.5542e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0786 - val_o1_acc: 0.9926 - val_o2_acc: 0.9874\n",
      "Epoch 99/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.7179e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0775 - val_o1_acc: 0.9924 - val_o2_acc: 0.9873\n",
      "Epoch 100/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.3263e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0792 - val_o1_acc: 0.9923 - val_o2_acc: 0.9871\n",
      "Epoch 101/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.1986e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0782 - val_o1_acc: 0.9928 - val_o2_acc: 0.9875\n",
      "Epoch 102/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.3368e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0796 - val_o1_acc: 0.9925 - val_o2_acc: 0.9874\n",
      "Epoch 103/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.5169e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0778 - val_o1_acc: 0.9924 - val_o2_acc: 0.9875\n",
      "Epoch 104/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 6.6917e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0798 - val_o1_acc: 0.9924 - val_o2_acc: 0.9872\n",
      "Epoch 105/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1733 - o2_loss: 7.3743e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0777 - val_o1_acc: 0.9928 - val_o2_acc: 0.9876\n",
      "Epoch 106/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1732 - o2_loss: 6.6973e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0775 - val_o1_acc: 0.9925 - val_o2_acc: 0.9873\n",
      "Epoch 107/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 7.0696e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0762 - val_o1_acc: 0.9930 - val_o2_acc: 0.9879\n",
      "Epoch 108/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1733 - o1_loss: 2.1732 - o2_loss: 6.8867e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0778 - val_o1_acc: 0.9929 - val_o2_acc: 0.9878\n",
      "Epoch 109/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.9161e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0776 - val_o1_acc: 0.9926 - val_o2_acc: 0.9874\n",
      "Epoch 110/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.6912e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0781 - val_o1_acc: 0.9925 - val_o2_acc: 0.9873\n",
      "Epoch 111/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.9974e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0768 - val_o1_acc: 0.9929 - val_o2_acc: 0.9876\n",
      "Epoch 112/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 7.1631e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0776 - val_o1_acc: 0.9926 - val_o2_acc: 0.9874\n",
      "Epoch 113/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.8113e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0785 - val_o1_acc: 0.9927 - val_o2_acc: 0.9878\n",
      "Epoch 114/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 7.0932e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0779 - val_o1_acc: 0.9930 - val_o2_acc: 0.9878\n",
      "Epoch 115/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.6816e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0802 - val_o1_acc: 0.9925 - val_o2_acc: 0.9873\n",
      "Epoch 116/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 7.0840e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0790 - val_o1_acc: 0.9926 - val_o2_acc: 0.9875\n",
      "Epoch 117/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.5118e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0785 - val_o1_acc: 0.9927 - val_o2_acc: 0.9875\n",
      "Epoch 118/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.7032e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0765 - val_o1_acc: 0.9927 - val_o2_acc: 0.9874\n",
      "Epoch 119/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.2077e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0768 - val_o1_acc: 0.9931 - val_o2_acc: 0.9880\n",
      "Epoch 120/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4865e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0770 - val_o1_acc: 0.9930 - val_o2_acc: 0.9877\n",
      "Epoch 121/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.2139e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0776 - val_o1_acc: 0.9928 - val_o2_acc: 0.9879\n",
      "Epoch 122/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.9357e-04 - o1_acc: 0.9990 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0783 - val_o1_acc: 0.9933 - val_o2_acc: 0.9882\n",
      "Epoch 123/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.2981e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0762 - val_o1_acc: 0.9933 - val_o2_acc: 0.9878\n",
      "Epoch 124/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.2743e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0767 - val_o1_acc: 0.9930 - val_o2_acc: 0.9877\n",
      "Epoch 125/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4268e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0760 - val_o1_acc: 0.9932 - val_o2_acc: 0.9881\n",
      "Epoch 126/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4039e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0777 - val_o1_acc: 0.9928 - val_o2_acc: 0.9879\n",
      "Epoch 127/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.7258e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0782 - val_o1_acc: 0.9930 - val_o2_acc: 0.9875\n",
      "Epoch 128/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4261e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0766 - val_o1_acc: 0.9928 - val_o2_acc: 0.9876\n",
      "Epoch 129/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4177e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0772 - val_o1_acc: 0.9932 - val_o2_acc: 0.9879\n",
      "Epoch 130/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4669e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0768 - val_o1_acc: 0.9926 - val_o2_acc: 0.9876\n",
      "Epoch 131/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.9516e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0765 - val_o1_acc: 0.9929 - val_o2_acc: 0.9875\n",
      "Epoch 132/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.0651e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0770 - val_o1_acc: 0.9930 - val_o2_acc: 0.9876\n",
      "Epoch 133/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.1444e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0780 - val_o1_acc: 0.9931 - val_o2_acc: 0.9878\n",
      "Epoch 134/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.3733e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0765 - val_o1_acc: 0.9928 - val_o2_acc: 0.9877\n",
      "Epoch 135/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4322e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0764 - val_o1_acc: 0.9929 - val_o2_acc: 0.9878\n",
      "Epoch 136/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.8670e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0770 - val_o1_acc: 0.9931 - val_o2_acc: 0.9880\n",
      "Epoch 137/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.9511e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0779 - val_o1_acc: 0.9928 - val_o2_acc: 0.9875\n",
      "Epoch 138/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.2581e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0767 - val_o1_acc: 0.9932 - val_o2_acc: 0.9879\n",
      "Epoch 139/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4741e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0758 - val_o1_acc: 0.9932 - val_o2_acc: 0.9880\n",
      "Epoch 140/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.9026e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0768 - val_o1_acc: 0.9927 - val_o2_acc: 0.9876\n",
      "Epoch 141/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.6537e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0768 - val_o1_acc: 0.9931 - val_o2_acc: 0.9878\n",
      "Epoch 142/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4883e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0774 - val_o1_acc: 0.9930 - val_o2_acc: 0.9874\n",
      "Epoch 143/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.3685e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0762 - val_o1_acc: 0.9931 - val_o2_acc: 0.9878\n",
      "Epoch 144/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.9605e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0779 - val_o1_acc: 0.9933 - val_o2_acc: 0.9883\n",
      "Epoch 145/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.0820e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0749 - val_o1_acc: 0.9935 - val_o2_acc: 0.9884\n",
      "Epoch 146/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.0044e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0764 - val_o1_acc: 0.9930 - val_o2_acc: 0.9877\n",
      "Epoch 147/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.9082e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0769 - val_o1_acc: 0.9931 - val_o2_acc: 0.9878\n",
      "Epoch 148/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.2334e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0754 - val_o1_acc: 0.9930 - val_o2_acc: 0.9877\n",
      "Epoch 149/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.8144e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0763 - val_o1_acc: 0.9932 - val_o2_acc: 0.9881\n",
      "Epoch 150/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.6398e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0772 - val_o1_acc: 0.9928 - val_o2_acc: 0.9877\n",
      "Epoch 151/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.6327e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0759 - val_o1_acc: 0.9931 - val_o2_acc: 0.9881\n",
      "Epoch 152/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.1858e-04 - o1_acc: 0.9989 - o2_acc: 0.9999 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0756 - val_o1_acc: 0.9934 - val_o2_acc: 0.9881\n",
      "Epoch 153/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.8005e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0762 - val_o1_acc: 0.9932 - val_o2_acc: 0.9881\n",
      "Epoch 154/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.9450e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0764 - val_o1_acc: 0.9935 - val_o2_acc: 0.9882\n",
      "Epoch 155/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.9335e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0764 - val_o1_acc: 0.9931 - val_o2_acc: 0.9880\n",
      "Epoch 156/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.1100e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0762 - val_o1_acc: 0.9936 - val_o2_acc: 0.9883\n",
      "Epoch 157/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.8061e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0750 - val_o1_acc: 0.9935 - val_o2_acc: 0.9884\n",
      "Epoch 158/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.9937e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0773 - val_o1_acc: 0.9929 - val_o2_acc: 0.9878\n",
      "Epoch 159/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.5768e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0777 - val_o1_acc: 0.9928 - val_o2_acc: 0.9877\n",
      "Epoch 160/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.9462e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0773 - val_o1_acc: 0.9932 - val_o2_acc: 0.9881\n",
      "Epoch 161/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.6276e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0765 - val_o1_acc: 0.9930 - val_o2_acc: 0.9877\n",
      "Epoch 162/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 6.4400e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0751 - val_o1_acc: 0.9932 - val_o2_acc: 0.9881\n",
      "Epoch 163/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1732 - o1_loss: 2.1732 - o2_loss: 5.8154e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0753 - val_o1_acc: 0.9929 - val_o2_acc: 0.9878\n",
      "Epoch 164/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.8117e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0765 - val_o1_acc: 0.9928 - val_o2_acc: 0.9879\n",
      "Epoch 165/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.6775e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0745 - val_o1_acc: 0.9931 - val_o2_acc: 0.9878\n",
      "Epoch 166/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.6707e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0770 - val_o1_acc: 0.9931 - val_o2_acc: 0.9880\n",
      "Epoch 167/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.5837e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0772 - val_o1_acc: 0.9929 - val_o2_acc: 0.9879\n",
      "Epoch 168/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.6701e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0768 - val_o1_acc: 0.9925 - val_o2_acc: 0.9876\n",
      "Epoch 169/200\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.7322e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0781 - val_o1_acc: 0.9925 - val_o2_acc: 0.9876\n",
      "Epoch 170/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.7045e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0758 - val_o1_acc: 0.9930 - val_o2_acc: 0.9879\n",
      "Epoch 171/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.5400e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0761 - val_o1_acc: 0.9932 - val_o2_acc: 0.9879\n",
      "Epoch 172/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 6.0490e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0750 - val_o1_acc: 0.9934 - val_o2_acc: 0.9883\n",
      "Epoch 173/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.8070e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0771 - val_o1_acc: 0.9930 - val_o2_acc: 0.9879\n",
      "Epoch 174/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.4129e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0760 - val_o1_acc: 0.9930 - val_o2_acc: 0.9881\n",
      "Epoch 175/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.6096e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0750 - val_o1_acc: 0.9930 - val_o2_acc: 0.9881\n",
      "Epoch 176/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.4251e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0757 - val_o1_acc: 0.9929 - val_o2_acc: 0.9878\n",
      "Epoch 177/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.3974e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0761 - val_o1_acc: 0.9931 - val_o2_acc: 0.9880\n",
      "Epoch 178/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.7956e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0751 - val_o1_acc: 0.9930 - val_o2_acc: 0.9884\n",
      "Epoch 179/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.7796e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0770 - val_o1_acc: 0.9930 - val_o2_acc: 0.9877\n",
      "Epoch 180/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.7617e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0745 - val_o1_acc: 0.9926 - val_o2_acc: 0.9876\n",
      "Epoch 181/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.6629e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0758 - val_o1_acc: 0.9928 - val_o2_acc: 0.9877\n",
      "Epoch 182/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.7043e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0771 - val_o1_acc: 0.9930 - val_o2_acc: 0.9879\n",
      "Epoch 183/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.3174e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0762 - val_o1_acc: 0.9932 - val_o2_acc: 0.9878\n",
      "Epoch 184/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.4642e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0769 - val_o1_acc: 0.9930 - val_o2_acc: 0.9881\n",
      "Epoch 185/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.6590e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0765 - val_o1_acc: 0.9928 - val_o2_acc: 0.9880\n",
      "Epoch 186/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.4063e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0753 - val_o1_acc: 0.9936 - val_o2_acc: 0.9883\n",
      "Epoch 187/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.5614e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0756 - val_o1_acc: 0.9929 - val_o2_acc: 0.9879\n",
      "Epoch 188/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.4308e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0751 - val_o1_acc: 0.9930 - val_o2_acc: 0.9881\n",
      "Epoch 189/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.5120e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0757 - val_o1_acc: 0.9931 - val_o2_acc: 0.9878\n",
      "Epoch 190/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.5894e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0768 - val_o1_acc: 0.9929 - val_o2_acc: 0.9878\n",
      "Epoch 191/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.3972e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0759 - val_o1_acc: 0.9930 - val_o2_acc: 0.9880\n",
      "Epoch 192/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.6983e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1725 - val_o2_loss: 0.0750 - val_o1_acc: 0.9929 - val_o2_acc: 0.9879\n",
      "Epoch 193/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.5431e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0755 - val_o1_acc: 0.9935 - val_o2_acc: 0.9883\n",
      "Epoch 194/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.4993e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0759 - val_o1_acc: 0.9929 - val_o2_acc: 0.9876\n",
      "Epoch 195/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.2755e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1727 - val_o1_loss: 2.1725 - val_o2_loss: 0.0763 - val_o1_acc: 0.9929 - val_o2_acc: 0.9879\n",
      "Epoch 196/200\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.4511e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0758 - val_o1_acc: 0.9934 - val_o2_acc: 0.9882\n",
      "Epoch 197/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.5944e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0758 - val_o1_acc: 0.9934 - val_o2_acc: 0.9883\n",
      "Epoch 198/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.3648e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0756 - val_o1_acc: 0.9932 - val_o2_acc: 0.9883\n",
      "Epoch 199/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.4448e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0753 - val_o1_acc: 0.9931 - val_o2_acc: 0.9880\n",
      "Epoch 200/200\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 2.1731 - o1_loss: 2.1731 - o2_loss: 5.5238e-04 - o1_acc: 0.9989 - o2_acc: 1.0000 - val_loss: 2.1726 - val_o1_loss: 2.1724 - val_o2_loss: 0.0763 - val_o1_acc: 0.9931 - val_o2_acc: 0.9880\n",
      "Test loss: 2.17262970123291\n",
      "Test accuracy: 0.988\n"
     ]
    }
   ],
   "source": [
    "''' REGULAR TEACHER IN CLASS '''\n",
    "reload(models)\n",
    "model['student_'] = models.StudentModel(input_shape, num_classes, T=T, in_class=True)\n",
    "model['student_'].compile(loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
    "                          loss_weights=[1., 1. / (T**2)],\n",
    "                          optimizer='Adam',\n",
    "                          metrics=['acc'])\n",
    "\n",
    "hist['student_'] = model['student_'].fit(x_train, [kd_gt['t_train'], y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, [kd_gt['t_test'], y_test]))\n",
    "score['student_'] = model['student_'].evaluate(x_test, [kd_gt['t_test'], y_test], verbose=0)\n",
    "print('Test loss:', score['student_'][0])\n",
    "print('Test accuracy:', score['student_'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.17262970123291\n",
      "Test accuracy: 0.988\n",
      "Test errors: 120\n"
     ]
    }
   ],
   "source": [
    "score['student_'] = model['student_'].evaluate(x_test, [kd_gt['t_test'], y_test], verbose=0)\n",
    "n_errors = np.int((1-score['student_'][-1])*len(y_test))\n",
    "print('Test loss:', score['student_'][0])\n",
    "print('Test accuracy:', score['student_'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 1.3890 - o1_loss: 1.7107 - o2_loss: 0.5336 - o1_acc: 0.8706 - o2_acc: 0.8704 - val_loss: 1.0678 - val_o1_loss: 1.5702 - val_o2_loss: 0.2827 - val_o1_acc: 0.9676 - val_o2_acc: 0.9661\n",
      "Epoch 2/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 1.0298 - o1_loss: 1.5495 - o2_loss: 0.2551 - o1_acc: 0.9731 - o2_acc: 0.9730 - val_loss: 1.0193 - val_o1_loss: 1.5620 - val_o2_loss: 0.2383 - val_o1_acc: 0.9768 - val_o2_acc: 0.9757\n",
      "Epoch 3/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9822 - o1_loss: 1.5307 - o2_loss: 0.2168 - o1_acc: 0.9853 - o2_acc: 0.9850 - val_loss: 0.9931 - val_o1_loss: 1.5167 - val_o2_loss: 0.2347 - val_o1_acc: 0.9815 - val_o2_acc: 0.9801\n",
      "Epoch 4/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9566 - o1_loss: 1.5198 - o2_loss: 0.1967 - o1_acc: 0.9909 - o2_acc: 0.9907 - val_loss: 0.9840 - val_o1_loss: 1.5267 - val_o2_loss: 0.2207 - val_o1_acc: 0.9845 - val_o2_acc: 0.9831\n",
      "Epoch 5/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9412 - o1_loss: 1.5138 - o2_loss: 0.1843 - o1_acc: 0.9948 - o2_acc: 0.9945 - val_loss: 0.9747 - val_o1_loss: 1.5327 - val_o2_loss: 0.2083 - val_o1_acc: 0.9858 - val_o2_acc: 0.9849\n",
      "Epoch 6/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9303 - o1_loss: 1.5097 - o2_loss: 0.1754 - o1_acc: 0.9975 - o2_acc: 0.9972 - val_loss: 0.9671 - val_o1_loss: 1.5256 - val_o2_loss: 0.2043 - val_o1_acc: 0.9865 - val_o2_acc: 0.9865\n",
      "Epoch 7/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9235 - o1_loss: 1.5072 - o2_loss: 0.1699 - o1_acc: 0.9986 - o2_acc: 0.9984 - val_loss: 0.9656 - val_o1_loss: 1.5260 - val_o2_loss: 0.2026 - val_o1_acc: 0.9866 - val_o2_acc: 0.9855\n",
      "Epoch 8/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9186 - o1_loss: 1.5048 - o2_loss: 0.1662 - o1_acc: 0.9993 - o2_acc: 0.9991 - val_loss: 0.9646 - val_o1_loss: 1.5209 - val_o2_loss: 0.2041 - val_o1_acc: 0.9868 - val_o2_acc: 0.9864\n",
      "Epoch 9/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9147 - o1_loss: 1.5032 - o2_loss: 0.1631 - o1_acc: 0.9995 - o2_acc: 0.9996 - val_loss: 0.9609 - val_o1_loss: 1.5128 - val_o2_loss: 0.2045 - val_o1_acc: 0.9868 - val_o2_acc: 0.9863\n",
      "Epoch 10/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9123 - o1_loss: 1.5023 - o2_loss: 0.1612 - o1_acc: 0.9996 - o2_acc: 0.9997 - val_loss: 0.9610 - val_o1_loss: 1.5118 - val_o2_loss: 0.2051 - val_o1_acc: 0.9869 - val_o2_acc: 0.9864\n",
      "Epoch 11/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9105 - o1_loss: 1.5010 - o2_loss: 0.1600 - o1_acc: 0.9997 - o2_acc: 0.9998 - val_loss: 0.9579 - val_o1_loss: 1.5136 - val_o2_loss: 0.2011 - val_o1_acc: 0.9872 - val_o2_acc: 0.9871\n",
      "Epoch 12/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9092 - o1_loss: 1.5006 - o2_loss: 0.1589 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9576 - val_o1_loss: 1.5101 - val_o2_loss: 0.2025 - val_o1_acc: 0.9875 - val_o2_acc: 0.9870\n",
      "Epoch 13/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9081 - o1_loss: 1.4999 - o2_loss: 0.1581 - o1_acc: 0.9997 - o2_acc: 1.0000 - val_loss: 0.9567 - val_o1_loss: 1.5220 - val_o2_loss: 0.1957 - val_o1_acc: 0.9876 - val_o2_acc: 0.9871\n",
      "Epoch 14/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9072 - o1_loss: 1.4996 - o2_loss: 0.1574 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9560 - val_o1_loss: 1.5081 - val_o2_loss: 0.2019 - val_o1_acc: 0.9868 - val_o2_acc: 0.9869\n",
      "Epoch 15/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9065 - o1_loss: 1.4990 - o2_loss: 0.1570 - o1_acc: 0.9997 - o2_acc: 1.0000 - val_loss: 0.9550 - val_o1_loss: 1.5071 - val_o2_loss: 0.2014 - val_o1_acc: 0.9872 - val_o2_acc: 0.9869\n",
      "Epoch 16/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9060 - o1_loss: 1.4988 - o2_loss: 0.1566 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9543 - val_o1_loss: 1.5142 - val_o2_loss: 0.1972 - val_o1_acc: 0.9870 - val_o2_acc: 0.9865\n",
      "Epoch 17/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9054 - o1_loss: 1.4984 - o2_loss: 0.1562 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9544 - val_o1_loss: 1.5024 - val_o2_loss: 0.2032 - val_o1_acc: 0.9872 - val_o2_acc: 0.9872\n",
      "Epoch 18/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9050 - o1_loss: 1.4980 - o2_loss: 0.1560 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9543 - val_o1_loss: 1.5082 - val_o2_loss: 0.2001 - val_o1_acc: 0.9871 - val_o2_acc: 0.9868\n",
      "Epoch 19/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9047 - o1_loss: 1.4979 - o2_loss: 0.1558 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9536 - val_o1_loss: 1.4985 - val_o2_loss: 0.2043 - val_o1_acc: 0.9877 - val_o2_acc: 0.9872\n",
      "Epoch 20/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9046 - o1_loss: 1.4977 - o2_loss: 0.1557 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9529 - val_o1_loss: 1.5021 - val_o2_loss: 0.2019 - val_o1_acc: 0.9873 - val_o2_acc: 0.9872\n",
      "Epoch 21/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9042 - o1_loss: 1.4974 - o2_loss: 0.1556 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9529 - val_o1_loss: 1.5134 - val_o2_loss: 0.1962 - val_o1_acc: 0.9876 - val_o2_acc: 0.9873\n",
      "Epoch 22/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9040 - o1_loss: 1.4973 - o2_loss: 0.1554 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9521 - val_o1_loss: 1.5099 - val_o2_loss: 0.1972 - val_o1_acc: 0.9878 - val_o2_acc: 0.9873\n",
      "Epoch 23/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9038 - o1_loss: 1.4972 - o2_loss: 0.1552 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9525 - val_o1_loss: 1.5032 - val_o2_loss: 0.2009 - val_o1_acc: 0.9874 - val_o2_acc: 0.9868\n",
      "Epoch 24/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9036 - o1_loss: 1.4970 - o2_loss: 0.1552 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9523 - val_o1_loss: 1.5034 - val_o2_loss: 0.2006 - val_o1_acc: 0.9873 - val_o2_acc: 0.9870\n",
      "Epoch 25/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9035 - o1_loss: 1.4968 - o2_loss: 0.1551 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9523 - val_o1_loss: 1.5069 - val_o2_loss: 0.1988 - val_o1_acc: 0.9872 - val_o2_acc: 0.9870\n",
      "Epoch 26/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9034 - o1_loss: 1.4968 - o2_loss: 0.1550 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9521 - val_o1_loss: 1.5016 - val_o2_loss: 0.2013 - val_o1_acc: 0.9875 - val_o2_acc: 0.9872\n",
      "Epoch 27/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9033 - o1_loss: 1.4966 - o2_loss: 0.1550 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9516 - val_o1_loss: 1.5030 - val_o2_loss: 0.2001 - val_o1_acc: 0.9874 - val_o2_acc: 0.9872\n",
      "Epoch 28/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9031 - o1_loss: 1.4965 - o2_loss: 0.1549 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9517 - val_o1_loss: 1.5050 - val_o2_loss: 0.1992 - val_o1_acc: 0.9872 - val_o2_acc: 0.9868\n",
      "Epoch 29/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9030 - o1_loss: 1.4963 - o2_loss: 0.1549 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9517 - val_o1_loss: 1.5094 - val_o2_loss: 0.1970 - val_o1_acc: 0.9871 - val_o2_acc: 0.9866\n",
      "Epoch 30/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9029 - o1_loss: 1.4963 - o2_loss: 0.1548 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9510 - val_o1_loss: 1.5077 - val_o2_loss: 0.1971 - val_o1_acc: 0.9871 - val_o2_acc: 0.9868\n",
      "Epoch 31/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9029 - o1_loss: 1.4962 - o2_loss: 0.1548 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9514 - val_o1_loss: 1.5063 - val_o2_loss: 0.1982 - val_o1_acc: 0.9877 - val_o2_acc: 0.9872\n",
      "Epoch 32/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9028 - o1_loss: 1.4961 - o2_loss: 0.1547 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9508 - val_o1_loss: 1.5063 - val_o2_loss: 0.1976 - val_o1_acc: 0.9872 - val_o2_acc: 0.9870\n",
      "Epoch 33/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9028 - o1_loss: 1.4961 - o2_loss: 0.1547 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9505 - val_o1_loss: 1.5104 - val_o2_loss: 0.1953 - val_o1_acc: 0.9874 - val_o2_acc: 0.9871\n",
      "Epoch 34/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9027 - o1_loss: 1.4960 - o2_loss: 0.1546 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9507 - val_o1_loss: 1.5012 - val_o2_loss: 0.2001 - val_o1_acc: 0.9876 - val_o2_acc: 0.9872\n",
      "Epoch 35/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9026 - o1_loss: 1.4959 - o2_loss: 0.1546 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9506 - val_o1_loss: 1.4994 - val_o2_loss: 0.2009 - val_o1_acc: 0.9876 - val_o2_acc: 0.9870\n",
      "Epoch 36/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9026 - o1_loss: 1.4958 - o2_loss: 0.1547 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9506 - val_o1_loss: 1.5040 - val_o2_loss: 0.1986 - val_o1_acc: 0.9879 - val_o2_acc: 0.9872\n",
      "Epoch 37/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9024 - o1_loss: 1.4958 - o2_loss: 0.1546 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9503 - val_o1_loss: 1.5042 - val_o2_loss: 0.1982 - val_o1_acc: 0.9874 - val_o2_acc: 0.9870\n",
      "Epoch 38/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9025 - o1_loss: 1.4958 - o2_loss: 0.1546 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9508 - val_o1_loss: 1.5021 - val_o2_loss: 0.1997 - val_o1_acc: 0.9876 - val_o2_acc: 0.9870\n",
      "Epoch 39/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9024 - o1_loss: 1.4956 - o2_loss: 0.1546 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9506 - val_o1_loss: 1.5054 - val_o2_loss: 0.1979 - val_o1_acc: 0.9874 - val_o2_acc: 0.9869\n",
      "Epoch 40/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9024 - o1_loss: 1.4957 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9501 - val_o1_loss: 1.5010 - val_o2_loss: 0.1996 - val_o1_acc: 0.9875 - val_o2_acc: 0.9873\n",
      "Epoch 41/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9023 - o1_loss: 1.4955 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9507 - val_o1_loss: 1.5009 - val_o2_loss: 0.2003 - val_o1_acc: 0.9876 - val_o2_acc: 0.9874\n",
      "Epoch 42/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9022 - o1_loss: 1.4954 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9503 - val_o1_loss: 1.4986 - val_o2_loss: 0.2010 - val_o1_acc: 0.9876 - val_o2_acc: 0.9874\n",
      "Epoch 43/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9022 - o1_loss: 1.4954 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9501 - val_o1_loss: 1.4957 - val_o2_loss: 0.2023 - val_o1_acc: 0.9877 - val_o2_acc: 0.9869\n",
      "Epoch 44/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9022 - o1_loss: 1.4953 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9501 - val_o1_loss: 1.5055 - val_o2_loss: 0.1974 - val_o1_acc: 0.9876 - val_o2_acc: 0.9873\n",
      "Epoch 45/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9021 - o1_loss: 1.4954 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9501 - val_o1_loss: 1.5042 - val_o2_loss: 0.1980 - val_o1_acc: 0.9879 - val_o2_acc: 0.9873\n",
      "Epoch 46/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9021 - o1_loss: 1.4953 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9500 - val_o1_loss: 1.5009 - val_o2_loss: 0.1996 - val_o1_acc: 0.9879 - val_o2_acc: 0.9871\n",
      "Epoch 47/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9021 - o1_loss: 1.4952 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9498 - val_o1_loss: 1.5095 - val_o2_loss: 0.1951 - val_o1_acc: 0.9877 - val_o2_acc: 0.9873\n",
      "Epoch 48/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9021 - o1_loss: 1.4954 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9497 - val_o1_loss: 1.5076 - val_o2_loss: 0.1959 - val_o1_acc: 0.9880 - val_o2_acc: 0.9878\n",
      "Epoch 49/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9021 - o1_loss: 1.4953 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9496 - val_o1_loss: 1.5012 - val_o2_loss: 0.1990 - val_o1_acc: 0.9878 - val_o2_acc: 0.9874\n",
      "Epoch 50/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9020 - o1_loss: 1.4952 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9497 - val_o1_loss: 1.4969 - val_o2_loss: 0.2012 - val_o1_acc: 0.9881 - val_o2_acc: 0.9875\n",
      "Epoch 51/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9020 - o1_loss: 1.4950 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9500 - val_o1_loss: 1.5084 - val_o2_loss: 0.1958 - val_o1_acc: 0.9879 - val_o2_acc: 0.9872\n",
      "Epoch 52/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9020 - o1_loss: 1.4952 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9497 - val_o1_loss: 1.4979 - val_o2_loss: 0.2008 - val_o1_acc: 0.9875 - val_o2_acc: 0.9871\n",
      "Epoch 53/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9020 - o1_loss: 1.4949 - o2_loss: 0.1545 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9498 - val_o1_loss: 1.5122 - val_o2_loss: 0.1937 - val_o1_acc: 0.9874 - val_o2_acc: 0.9871\n",
      "Epoch 54/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9020 - o1_loss: 1.4952 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9497 - val_o1_loss: 1.5032 - val_o2_loss: 0.1981 - val_o1_acc: 0.9880 - val_o2_acc: 0.9876\n",
      "Epoch 55/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9019 - o1_loss: 1.4951 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9497 - val_o1_loss: 1.5013 - val_o2_loss: 0.1991 - val_o1_acc: 0.9876 - val_o2_acc: 0.9872\n",
      "Epoch 56/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9019 - o1_loss: 1.4950 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9495 - val_o1_loss: 1.4970 - val_o2_loss: 0.2009 - val_o1_acc: 0.9879 - val_o2_acc: 0.9876\n",
      "Epoch 57/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9018 - o1_loss: 1.4948 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9500 - val_o1_loss: 1.5060 - val_o2_loss: 0.1970 - val_o1_acc: 0.9879 - val_o2_acc: 0.9873\n",
      "Epoch 58/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9018 - o1_loss: 1.4950 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9495 - val_o1_loss: 1.4932 - val_o2_loss: 0.2029 - val_o1_acc: 0.9878 - val_o2_acc: 0.9872\n",
      "Epoch 59/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9019 - o1_loss: 1.4949 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9496 - val_o1_loss: 1.4974 - val_o2_loss: 0.2009 - val_o1_acc: 0.9871 - val_o2_acc: 0.9873\n",
      "Epoch 60/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9018 - o1_loss: 1.4949 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9497 - val_o1_loss: 1.4983 - val_o2_loss: 0.2006 - val_o1_acc: 0.9874 - val_o2_acc: 0.9873\n",
      "Epoch 61/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9018 - o1_loss: 1.4948 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9492 - val_o1_loss: 1.5019 - val_o2_loss: 0.1982 - val_o1_acc: 0.9876 - val_o2_acc: 0.9872\n",
      "Epoch 62/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9018 - o1_loss: 1.4948 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9494 - val_o1_loss: 1.5077 - val_o2_loss: 0.1956 - val_o1_acc: 0.9879 - val_o2_acc: 0.9876\n",
      "Epoch 63/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9017 - o1_loss: 1.4949 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9495 - val_o1_loss: 1.4981 - val_o2_loss: 0.2004 - val_o1_acc: 0.9878 - val_o2_acc: 0.9873\n",
      "Epoch 64/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9017 - o1_loss: 1.4947 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9494 - val_o1_loss: 1.5051 - val_o2_loss: 0.1968 - val_o1_acc: 0.9877 - val_o2_acc: 0.9876\n",
      "Epoch 65/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9017 - o1_loss: 1.4948 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9495 - val_o1_loss: 1.4930 - val_o2_loss: 0.2030 - val_o1_acc: 0.9877 - val_o2_acc: 0.9873\n",
      "Epoch 66/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9017 - o1_loss: 1.4947 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9492 - val_o1_loss: 1.4979 - val_o2_loss: 0.2002 - val_o1_acc: 0.9876 - val_o2_acc: 0.9874\n",
      "Epoch 67/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9016 - o1_loss: 1.4946 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9492 - val_o1_loss: 1.5015 - val_o2_loss: 0.1985 - val_o1_acc: 0.9880 - val_o2_acc: 0.9875\n",
      "Epoch 68/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9016 - o1_loss: 1.4946 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9492 - val_o1_loss: 1.4979 - val_o2_loss: 0.2003 - val_o1_acc: 0.9876 - val_o2_acc: 0.9872\n",
      "Epoch 69/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9016 - o1_loss: 1.4945 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9495 - val_o1_loss: 1.5017 - val_o2_loss: 0.1986 - val_o1_acc: 0.9878 - val_o2_acc: 0.9873\n",
      "Epoch 70/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9017 - o1_loss: 1.4947 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9492 - val_o1_loss: 1.4966 - val_o2_loss: 0.2009 - val_o1_acc: 0.9875 - val_o2_acc: 0.9877\n",
      "Epoch 71/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9016 - o1_loss: 1.4945 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9498 - val_o1_loss: 1.5032 - val_o2_loss: 0.1982 - val_o1_acc: 0.9876 - val_o2_acc: 0.9870\n",
      "Epoch 72/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9016 - o1_loss: 1.4946 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9486 - val_o1_loss: 1.4980 - val_o2_loss: 0.1996 - val_o1_acc: 0.9876 - val_o2_acc: 0.9875\n",
      "Epoch 73/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9017 - o1_loss: 1.4945 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9488 - val_o1_loss: 1.5074 - val_o2_loss: 0.1951 - val_o1_acc: 0.9874 - val_o2_acc: 0.9870\n",
      "Epoch 74/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9016 - o1_loss: 1.4947 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9489 - val_o1_loss: 1.4995 - val_o2_loss: 0.1991 - val_o1_acc: 0.9878 - val_o2_acc: 0.9876\n",
      "Epoch 75/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9017 - o1_loss: 1.4946 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9492 - val_o1_loss: 1.5014 - val_o2_loss: 0.1985 - val_o1_acc: 0.9877 - val_o2_acc: 0.9876\n",
      "Epoch 76/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9017 - o1_loss: 1.4947 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9495 - val_o1_loss: 1.5032 - val_o2_loss: 0.1979 - val_o1_acc: 0.9875 - val_o2_acc: 0.9875\n",
      "Epoch 77/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9017 - o1_loss: 1.4946 - o2_loss: 0.1544 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9491 - val_o1_loss: 1.5075 - val_o2_loss: 0.1953 - val_o1_acc: 0.9874 - val_o2_acc: 0.9870\n",
      "Epoch 78/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9015 - o1_loss: 1.4946 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9493 - val_o1_loss: 1.4968 - val_o2_loss: 0.2009 - val_o1_acc: 0.9878 - val_o2_acc: 0.9874\n",
      "Epoch 79/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9015 - o1_loss: 1.4944 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9487 - val_o1_loss: 1.4977 - val_o2_loss: 0.1999 - val_o1_acc: 0.9875 - val_o2_acc: 0.9877\n",
      "Epoch 80/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9015 - o1_loss: 1.4944 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9485 - val_o1_loss: 1.5054 - val_o2_loss: 0.1958 - val_o1_acc: 0.9880 - val_o2_acc: 0.9875\n",
      "Epoch 81/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9015 - o1_loss: 1.4944 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9490 - val_o1_loss: 1.5076 - val_o2_loss: 0.1952 - val_o1_acc: 0.9874 - val_o2_acc: 0.9874\n",
      "Epoch 82/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9014 - o1_loss: 1.4943 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9490 - val_o1_loss: 1.5066 - val_o2_loss: 0.1957 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 83/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9014 - o1_loss: 1.4943 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9486 - val_o1_loss: 1.4998 - val_o2_loss: 0.1987 - val_o1_acc: 0.9873 - val_o2_acc: 0.9873\n",
      "Epoch 84/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9015 - o1_loss: 1.4943 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9491 - val_o1_loss: 1.5040 - val_o2_loss: 0.1971 - val_o1_acc: 0.9876 - val_o2_acc: 0.9875\n",
      "Epoch 85/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9014 - o1_loss: 1.4943 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9487 - val_o1_loss: 1.5010 - val_o2_loss: 0.1982 - val_o1_acc: 0.9877 - val_o2_acc: 0.9876\n",
      "Epoch 86/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9014 - o1_loss: 1.4942 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9489 - val_o1_loss: 1.5075 - val_o2_loss: 0.1951 - val_o1_acc: 0.9875 - val_o2_acc: 0.9873\n",
      "Epoch 87/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9014 - o1_loss: 1.4943 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9484 - val_o1_loss: 1.5042 - val_o2_loss: 0.1962 - val_o1_acc: 0.9874 - val_o2_acc: 0.9875\n",
      "Epoch 88/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9013 - o1_loss: 1.4942 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9485 - val_o1_loss: 1.4996 - val_o2_loss: 0.1987 - val_o1_acc: 0.9879 - val_o2_acc: 0.9875\n",
      "Epoch 89/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9013 - o1_loss: 1.4942 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9486 - val_o1_loss: 1.4990 - val_o2_loss: 0.1991 - val_o1_acc: 0.9877 - val_o2_acc: 0.9875\n",
      "Epoch 90/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9014 - o1_loss: 1.4942 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9486 - val_o1_loss: 1.5053 - val_o2_loss: 0.1960 - val_o1_acc: 0.9878 - val_o2_acc: 0.9874\n",
      "Epoch 91/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9015 - o1_loss: 1.4942 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5086 - val_o2_loss: 0.1939 - val_o1_acc: 0.9876 - val_o2_acc: 0.9873\n",
      "Epoch 92/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9015 - o1_loss: 1.4944 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9485 - val_o1_loss: 1.5010 - val_o2_loss: 0.1980 - val_o1_acc: 0.9874 - val_o2_acc: 0.9875\n",
      "Epoch 93/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9015 - o1_loss: 1.4944 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9486 - val_o1_loss: 1.4953 - val_o2_loss: 0.2010 - val_o1_acc: 0.9877 - val_o2_acc: 0.9875\n",
      "Epoch 94/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9014 - o1_loss: 1.4941 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9483 - val_o1_loss: 1.5048 - val_o2_loss: 0.1959 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 95/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9013 - o1_loss: 1.4942 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5034 - val_o2_loss: 0.1965 - val_o1_acc: 0.9877 - val_o2_acc: 0.9872\n",
      "Epoch 96/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9013 - o1_loss: 1.4941 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9486 - val_o1_loss: 1.5051 - val_o2_loss: 0.1961 - val_o1_acc: 0.9880 - val_o2_acc: 0.9878\n",
      "Epoch 97/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9013 - o1_loss: 1.4942 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9484 - val_o1_loss: 1.5003 - val_o2_loss: 0.1982 - val_o1_acc: 0.9877 - val_o2_acc: 0.9874\n",
      "Epoch 98/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9013 - o1_loss: 1.4941 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9487 - val_o1_loss: 1.4999 - val_o2_loss: 0.1987 - val_o1_acc: 0.9875 - val_o2_acc: 0.9872\n",
      "Epoch 99/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9013 - o1_loss: 1.4941 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9484 - val_o1_loss: 1.4983 - val_o2_loss: 0.1993 - val_o1_acc: 0.9879 - val_o2_acc: 0.9871\n",
      "Epoch 100/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9013 - o1_loss: 1.4941 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9484 - val_o1_loss: 1.4981 - val_o2_loss: 0.1993 - val_o1_acc: 0.9877 - val_o2_acc: 0.9871\n",
      "Epoch 101/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4940 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9483 - val_o1_loss: 1.4963 - val_o2_loss: 0.2001 - val_o1_acc: 0.9876 - val_o2_acc: 0.9873\n",
      "Epoch 102/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4940 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.4983 - val_o2_loss: 0.1991 - val_o1_acc: 0.9877 - val_o2_acc: 0.9872\n",
      "Epoch 103/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4940 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9483 - val_o1_loss: 1.4938 - val_o2_loss: 0.2013 - val_o1_acc: 0.9877 - val_o2_acc: 0.9873\n",
      "Epoch 104/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4939 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5027 - val_o2_loss: 0.1969 - val_o1_acc: 0.9878 - val_o2_acc: 0.9871\n",
      "Epoch 105/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4940 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.5029 - val_o2_loss: 0.1967 - val_o1_acc: 0.9878 - val_o2_acc: 0.9876\n",
      "Epoch 106/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4941 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4985 - val_o2_loss: 0.1986 - val_o1_acc: 0.9878 - val_o2_acc: 0.9874\n",
      "Epoch 107/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4940 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5029 - val_o2_loss: 0.1967 - val_o1_acc: 0.9875 - val_o2_acc: 0.9876\n",
      "Epoch 108/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9013 - o1_loss: 1.4940 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4999 - val_o2_loss: 0.1979 - val_o1_acc: 0.9884 - val_o2_acc: 0.9877\n",
      "Epoch 109/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4939 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5052 - val_o2_loss: 0.1956 - val_o1_acc: 0.9878 - val_o2_acc: 0.9874\n",
      "Epoch 110/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9013 - o1_loss: 1.4941 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.5008 - val_o2_loss: 0.1976 - val_o1_acc: 0.9875 - val_o2_acc: 0.9870\n",
      "Epoch 111/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4940 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4994 - val_o2_loss: 0.1982 - val_o1_acc: 0.9878 - val_o2_acc: 0.9874\n",
      "Epoch 112/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4939 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9483 - val_o1_loss: 1.5006 - val_o2_loss: 0.1980 - val_o1_acc: 0.9879 - val_o2_acc: 0.9872\n",
      "Epoch 113/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4940 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4980 - val_o2_loss: 0.1986 - val_o1_acc: 0.9878 - val_o2_acc: 0.9874\n",
      "Epoch 114/300\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.9012 - o1_loss: 1.4939 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5009 - val_o2_loss: 0.1972 - val_o1_acc: 0.9877 - val_o2_acc: 0.9872\n",
      "Epoch 115/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4939 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5056 - val_o2_loss: 0.1954 - val_o1_acc: 0.9880 - val_o2_acc: 0.9871\n",
      "Epoch 116/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4940 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9484 - val_o1_loss: 1.4970 - val_o2_loss: 0.1999 - val_o1_acc: 0.9880 - val_o2_acc: 0.9872\n",
      "Epoch 117/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4938 - o2_loss: 0.1543 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5054 - val_o2_loss: 0.1953 - val_o1_acc: 0.9876 - val_o2_acc: 0.9872\n",
      "Epoch 118/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9012 - o1_loss: 1.4940 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9483 - val_o1_loss: 1.4970 - val_o2_loss: 0.1997 - val_o1_acc: 0.9875 - val_o2_acc: 0.9872\n",
      "Epoch 119/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9011 - o1_loss: 1.4938 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4995 - val_o2_loss: 0.1982 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 120/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9011 - o1_loss: 1.4937 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5031 - val_o2_loss: 0.1964 - val_o1_acc: 0.9879 - val_o2_acc: 0.9873\n",
      "Epoch 121/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4938 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5001 - val_o2_loss: 0.1979 - val_o1_acc: 0.9881 - val_o2_acc: 0.9873\n",
      "Epoch 122/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4938 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4966 - val_o2_loss: 0.1997 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 123/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4936 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5001 - val_o2_loss: 0.1982 - val_o1_acc: 0.9877 - val_o2_acc: 0.9874\n",
      "Epoch 124/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4937 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.5018 - val_o2_loss: 0.1972 - val_o1_acc: 0.9876 - val_o2_acc: 0.9869\n",
      "Epoch 125/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4937 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5046 - val_o2_loss: 0.1957 - val_o1_acc: 0.9877 - val_o2_acc: 0.9872\n",
      "Epoch 126/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4937 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5016 - val_o2_loss: 0.1971 - val_o1_acc: 0.9877 - val_o2_acc: 0.9872\n",
      "Epoch 127/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4936 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5006 - val_o2_loss: 0.1977 - val_o1_acc: 0.9877 - val_o2_acc: 0.9870\n",
      "Epoch 128/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4936 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5005 - val_o2_loss: 0.1977 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 129/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4937 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.4987 - val_o2_loss: 0.1988 - val_o1_acc: 0.9879 - val_o2_acc: 0.9872\n",
      "Epoch 130/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4936 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5036 - val_o2_loss: 0.1961 - val_o1_acc: 0.9879 - val_o2_acc: 0.9873\n",
      "Epoch 131/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4937 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.5010 - val_o2_loss: 0.1976 - val_o1_acc: 0.9876 - val_o2_acc: 0.9871\n",
      "Epoch 132/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4937 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5000 - val_o2_loss: 0.1980 - val_o1_acc: 0.9879 - val_o2_acc: 0.9870\n",
      "Epoch 133/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4937 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5003 - val_o2_loss: 0.1980 - val_o1_acc: 0.9881 - val_o2_acc: 0.9874\n",
      "Epoch 134/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4936 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9486 - val_o1_loss: 1.5005 - val_o2_loss: 0.1984 - val_o1_acc: 0.9878 - val_o2_acc: 0.9875\n",
      "Epoch 135/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4936 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5015 - val_o2_loss: 0.1969 - val_o1_acc: 0.9880 - val_o2_acc: 0.9874\n",
      "Epoch 136/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9010 - o1_loss: 1.4936 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.4966 - val_o2_loss: 0.1994 - val_o1_acc: 0.9877 - val_o2_acc: 0.9877\n",
      "Epoch 137/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4935 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9484 - val_o1_loss: 1.5036 - val_o2_loss: 0.1966 - val_o1_acc: 0.9877 - val_o2_acc: 0.9872\n",
      "Epoch 138/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4936 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4974 - val_o2_loss: 0.1992 - val_o1_acc: 0.9877 - val_o2_acc: 0.9870\n",
      "Epoch 139/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4935 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9483 - val_o1_loss: 1.4996 - val_o2_loss: 0.1985 - val_o1_acc: 0.9882 - val_o2_acc: 0.9877\n",
      "Epoch 140/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4935 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9475 - val_o1_loss: 1.5047 - val_o2_loss: 0.1951 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 141/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4936 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4979 - val_o2_loss: 0.1989 - val_o1_acc: 0.9877 - val_o2_acc: 0.9873\n",
      "Epoch 142/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4934 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5025 - val_o2_loss: 0.1964 - val_o1_acc: 0.9880 - val_o2_acc: 0.9874\n",
      "Epoch 143/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4935 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5028 - val_o2_loss: 0.1963 - val_o1_acc: 0.9877 - val_o2_acc: 0.9872\n",
      "Epoch 144/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4936 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.4987 - val_o2_loss: 0.1984 - val_o1_acc: 0.9878 - val_o2_acc: 0.9873\n",
      "Epoch 145/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4935 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5026 - val_o2_loss: 0.1969 - val_o1_acc: 0.9878 - val_o2_acc: 0.9871\n",
      "Epoch 146/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4936 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9473 - val_o1_loss: 1.5004 - val_o2_loss: 0.1971 - val_o1_acc: 0.9878 - val_o2_acc: 0.9872\n",
      "Epoch 147/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4935 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.5020 - val_o2_loss: 0.1972 - val_o1_acc: 0.9880 - val_o2_acc: 0.9873\n",
      "Epoch 148/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4936 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.4961 - val_o2_loss: 0.1997 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 149/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4935 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.4982 - val_o2_loss: 0.1986 - val_o1_acc: 0.9880 - val_o2_acc: 0.9873\n",
      "Epoch 150/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4935 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5012 - val_o2_loss: 0.1973 - val_o1_acc: 0.9880 - val_o2_acc: 0.9873\n",
      "Epoch 151/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4934 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5022 - val_o2_loss: 0.1965 - val_o1_acc: 0.9884 - val_o2_acc: 0.9873\n",
      "Epoch 152/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4934 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5010 - val_o2_loss: 0.1973 - val_o1_acc: 0.9881 - val_o2_acc: 0.9874\n",
      "Epoch 153/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4935 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4985 - val_o2_loss: 0.1983 - val_o1_acc: 0.9880 - val_o2_acc: 0.9873\n",
      "Epoch 154/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5017 - val_o2_loss: 0.1969 - val_o1_acc: 0.9879 - val_o2_acc: 0.9873\n",
      "Epoch 155/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4934 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9474 - val_o1_loss: 1.5006 - val_o2_loss: 0.1970 - val_o1_acc: 0.9880 - val_o2_acc: 0.9875\n",
      "Epoch 156/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5039 - val_o2_loss: 0.1957 - val_o1_acc: 0.9883 - val_o2_acc: 0.9874\n",
      "Epoch 157/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4934 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5035 - val_o2_loss: 0.1961 - val_o1_acc: 0.9881 - val_o2_acc: 0.9876\n",
      "Epoch 158/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4934 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5004 - val_o2_loss: 0.1974 - val_o1_acc: 0.9879 - val_o2_acc: 0.9876\n",
      "Epoch 159/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4934 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4974 - val_o2_loss: 0.1993 - val_o1_acc: 0.9879 - val_o2_acc: 0.9872\n",
      "Epoch 160/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5001 - val_o2_loss: 0.1979 - val_o1_acc: 0.9876 - val_o2_acc: 0.9871\n",
      "Epoch 161/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4998 - val_o2_loss: 0.1979 - val_o1_acc: 0.9883 - val_o2_acc: 0.9875\n",
      "Epoch 162/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5022 - val_o2_loss: 0.1965 - val_o1_acc: 0.9880 - val_o2_acc: 0.9876\n",
      "Epoch 163/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4998 - val_o2_loss: 0.1976 - val_o1_acc: 0.9880 - val_o2_acc: 0.9871\n",
      "Epoch 164/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5006 - val_o2_loss: 0.1974 - val_o1_acc: 0.9877 - val_o2_acc: 0.9874\n",
      "Epoch 165/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4934 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4986 - val_o2_loss: 0.1988 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 166/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4934 - o2_loss: 0.1542 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9483 - val_o1_loss: 1.4996 - val_o2_loss: 0.1985 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 167/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9009 - o1_loss: 1.4935 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4969 - val_o2_loss: 0.1996 - val_o1_acc: 0.9881 - val_o2_acc: 0.9876\n",
      "Epoch 168/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4934 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4976 - val_o2_loss: 0.1993 - val_o1_acc: 0.9882 - val_o2_acc: 0.9876\n",
      "Epoch 169/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4988 - val_o2_loss: 0.1985 - val_o1_acc: 0.9880 - val_o2_acc: 0.9873\n",
      "Epoch 170/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9474 - val_o1_loss: 1.4998 - val_o2_loss: 0.1975 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 171/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9008 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5011 - val_o2_loss: 0.1970 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 172/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5030 - val_o2_loss: 0.1963 - val_o1_acc: 0.9880 - val_o2_acc: 0.9874\n",
      "Epoch 173/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4933 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.4975 - val_o2_loss: 0.1990 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 174/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9475 - val_o1_loss: 1.5000 - val_o2_loss: 0.1975 - val_o1_acc: 0.9879 - val_o2_acc: 0.9874\n",
      "Epoch 175/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5007 - val_o2_loss: 0.1974 - val_o1_acc: 0.9883 - val_o2_acc: 0.9878\n",
      "Epoch 176/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4986 - val_o2_loss: 0.1983 - val_o1_acc: 0.9881 - val_o2_acc: 0.9876\n",
      "Epoch 177/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4984 - val_o2_loss: 0.1985 - val_o1_acc: 0.9881 - val_o2_acc: 0.9874\n",
      "Epoch 178/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9475 - val_o1_loss: 1.4994 - val_o2_loss: 0.1978 - val_o1_acc: 0.9878 - val_o2_acc: 0.9871\n",
      "Epoch 179/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5007 - val_o2_loss: 0.1972 - val_o1_acc: 0.9881 - val_o2_acc: 0.9874\n",
      "Epoch 180/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5005 - val_o2_loss: 0.1977 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 181/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4992 - val_o2_loss: 0.1983 - val_o1_acc: 0.9881 - val_o2_acc: 0.9874\n",
      "Epoch 182/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4998 - val_o2_loss: 0.1977 - val_o1_acc: 0.9881 - val_o2_acc: 0.9873\n",
      "Epoch 183/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4969 - val_o2_loss: 0.1994 - val_o1_acc: 0.9880 - val_o2_acc: 0.9876\n",
      "Epoch 184/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.4974 - val_o2_loss: 0.1990 - val_o1_acc: 0.9879 - val_o2_acc: 0.9876\n",
      "Epoch 185/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5009 - val_o2_loss: 0.1972 - val_o1_acc: 0.9883 - val_o2_acc: 0.9878\n",
      "Epoch 186/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.4990 - val_o2_loss: 0.1982 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 187/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4986 - val_o2_loss: 0.1987 - val_o1_acc: 0.9882 - val_o2_acc: 0.9876\n",
      "Epoch 188/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4981 - val_o2_loss: 0.1987 - val_o1_acc: 0.9880 - val_o2_acc: 0.9876\n",
      "Epoch 189/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9007 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9475 - val_o1_loss: 1.5009 - val_o2_loss: 0.1971 - val_o1_acc: 0.9882 - val_o2_acc: 0.9873\n",
      "Epoch 190/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5006 - val_o2_loss: 0.1972 - val_o1_acc: 0.9881 - val_o2_acc: 0.9877\n",
      "Epoch 191/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9475 - val_o1_loss: 1.5012 - val_o2_loss: 0.1970 - val_o1_acc: 0.9880 - val_o2_acc: 0.9877\n",
      "Epoch 192/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5005 - val_o2_loss: 0.1974 - val_o1_acc: 0.9881 - val_o2_acc: 0.9878\n",
      "Epoch 193/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5022 - val_o2_loss: 0.1967 - val_o1_acc: 0.9883 - val_o2_acc: 0.9876\n",
      "Epoch 194/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9007 - o1_loss: 1.4932 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4988 - val_o2_loss: 0.1982 - val_o1_acc: 0.9881 - val_o2_acc: 0.9878\n",
      "Epoch 195/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5007 - val_o2_loss: 0.1973 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 196/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4988 - val_o2_loss: 0.1982 - val_o1_acc: 0.9882 - val_o2_acc: 0.9877\n",
      "Epoch 197/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4985 - val_o2_loss: 0.1986 - val_o1_acc: 0.9885 - val_o2_acc: 0.9875\n",
      "Epoch 198/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9475 - val_o1_loss: 1.5002 - val_o2_loss: 0.1974 - val_o1_acc: 0.9881 - val_o2_acc: 0.9876\n",
      "Epoch 199/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9475 - val_o1_loss: 1.5003 - val_o2_loss: 0.1974 - val_o1_acc: 0.9886 - val_o2_acc: 0.9877\n",
      "Epoch 200/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4970 - val_o2_loss: 0.1991 - val_o1_acc: 0.9878 - val_o2_acc: 0.9873\n",
      "Epoch 201/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5014 - val_o2_loss: 0.1971 - val_o1_acc: 0.9882 - val_o2_acc: 0.9876\n",
      "Epoch 202/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9475 - val_o1_loss: 1.5003 - val_o2_loss: 0.1974 - val_o1_acc: 0.9882 - val_o2_acc: 0.9873\n",
      "Epoch 203/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5003 - val_o2_loss: 0.1974 - val_o1_acc: 0.9883 - val_o2_acc: 0.9876\n",
      "Epoch 204/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4980 - val_o2_loss: 0.1986 - val_o1_acc: 0.9882 - val_o2_acc: 0.9873\n",
      "Epoch 205/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4992 - val_o2_loss: 0.1980 - val_o1_acc: 0.9881 - val_o2_acc: 0.9877\n",
      "Epoch 206/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9474 - val_o1_loss: 1.4990 - val_o2_loss: 0.1979 - val_o1_acc: 0.9883 - val_o2_acc: 0.9876\n",
      "Epoch 207/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5002 - val_o2_loss: 0.1976 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 208/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.5012 - val_o2_loss: 0.1970 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 209/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4992 - val_o2_loss: 0.1982 - val_o1_acc: 0.9883 - val_o2_acc: 0.9877\n",
      "Epoch 210/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4987 - val_o2_loss: 0.1983 - val_o1_acc: 0.9881 - val_o2_acc: 0.9875\n",
      "Epoch 211/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5019 - val_o2_loss: 0.1967 - val_o1_acc: 0.9881 - val_o2_acc: 0.9875\n",
      "Epoch 212/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4931 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4996 - val_o2_loss: 0.1978 - val_o1_acc: 0.9881 - val_o2_acc: 0.9873\n",
      "Epoch 213/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5001 - val_o2_loss: 0.1978 - val_o1_acc: 0.9882 - val_o2_acc: 0.9876\n",
      "Epoch 214/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4991 - val_o2_loss: 0.1983 - val_o1_acc: 0.9881 - val_o2_acc: 0.9873\n",
      "Epoch 215/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5003 - val_o2_loss: 0.1976 - val_o1_acc: 0.9885 - val_o2_acc: 0.9877\n",
      "Epoch 216/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4992 - val_o2_loss: 0.1982 - val_o1_acc: 0.9882 - val_o2_acc: 0.9877\n",
      "Epoch 217/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4998 - val_o2_loss: 0.1977 - val_o1_acc: 0.9884 - val_o2_acc: 0.9876\n",
      "Epoch 218/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4996 - val_o2_loss: 0.1981 - val_o1_acc: 0.9879 - val_o2_acc: 0.9875\n",
      "Epoch 219/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5018 - val_o2_loss: 0.1969 - val_o1_acc: 0.9884 - val_o2_acc: 0.9877\n",
      "Epoch 220/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4980 - val_o2_loss: 0.1988 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 221/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5013 - val_o2_loss: 0.1972 - val_o1_acc: 0.9882 - val_o2_acc: 0.9877\n",
      "Epoch 222/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5008 - val_o2_loss: 0.1975 - val_o1_acc: 0.9882 - val_o2_acc: 0.9874\n",
      "Epoch 223/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4992 - val_o2_loss: 0.1983 - val_o1_acc: 0.9881 - val_o2_acc: 0.9873\n",
      "Epoch 224/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.4989 - val_o2_loss: 0.1983 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 225/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4993 - val_o2_loss: 0.1980 - val_o1_acc: 0.9885 - val_o2_acc: 0.9874\n",
      "Epoch 226/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4992 - val_o2_loss: 0.1982 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 227/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.4998 - val_o2_loss: 0.1978 - val_o1_acc: 0.9889 - val_o2_acc: 0.9877\n",
      "Epoch 228/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5012 - val_o2_loss: 0.1971 - val_o1_acc: 0.9881 - val_o2_acc: 0.9877\n",
      "Epoch 229/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5011 - val_o2_loss: 0.1971 - val_o1_acc: 0.9884 - val_o2_acc: 0.9879\n",
      "Epoch 230/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5005 - val_o2_loss: 0.1976 - val_o1_acc: 0.9881 - val_o2_acc: 0.9874\n",
      "Epoch 231/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9476 - val_o1_loss: 1.4982 - val_o2_loss: 0.1985 - val_o1_acc: 0.9883 - val_o2_acc: 0.9878\n",
      "Epoch 232/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5003 - val_o2_loss: 0.1977 - val_o1_acc: 0.9883 - val_o2_acc: 0.9873\n",
      "Epoch 233/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9006 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5006 - val_o2_loss: 0.1975 - val_o1_acc: 0.9886 - val_o2_acc: 0.9878\n",
      "Epoch 234/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4930 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5006 - val_o2_loss: 0.1975 - val_o1_acc: 0.9883 - val_o2_acc: 0.9874\n",
      "Epoch 235/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4999 - val_o2_loss: 0.1979 - val_o1_acc: 0.9885 - val_o2_acc: 0.9872\n",
      "Epoch 236/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5006 - val_o2_loss: 0.1974 - val_o1_acc: 0.9885 - val_o2_acc: 0.9874\n",
      "Epoch 237/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5008 - val_o2_loss: 0.1975 - val_o1_acc: 0.9881 - val_o2_acc: 0.9875\n",
      "Epoch 238/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5002 - val_o2_loss: 0.1978 - val_o1_acc: 0.9885 - val_o2_acc: 0.9875\n",
      "Epoch 239/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4992 - val_o2_loss: 0.1982 - val_o1_acc: 0.9885 - val_o2_acc: 0.9872\n",
      "Epoch 240/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4990 - val_o2_loss: 0.1984 - val_o1_acc: 0.9886 - val_o2_acc: 0.9876\n",
      "Epoch 241/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4993 - val_o2_loss: 0.1983 - val_o1_acc: 0.9883 - val_o2_acc: 0.9873\n",
      "Epoch 242/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4991 - val_o2_loss: 0.1983 - val_o1_acc: 0.9884 - val_o2_acc: 0.9875\n",
      "Epoch 243/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5002 - val_o2_loss: 0.1979 - val_o1_acc: 0.9883 - val_o2_acc: 0.9876\n",
      "Epoch 244/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4987 - val_o2_loss: 0.1984 - val_o1_acc: 0.9888 - val_o2_acc: 0.9877\n",
      "Epoch 245/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4994 - val_o2_loss: 0.1981 - val_o1_acc: 0.9886 - val_o2_acc: 0.9872\n",
      "Epoch 246/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5004 - val_o2_loss: 0.1977 - val_o1_acc: 0.9884 - val_o2_acc: 0.9877\n",
      "Epoch 247/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5002 - val_o2_loss: 0.1978 - val_o1_acc: 0.9879 - val_o2_acc: 0.9875\n",
      "Epoch 248/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4999 - val_o2_loss: 0.1980 - val_o1_acc: 0.9884 - val_o2_acc: 0.9874\n",
      "Epoch 249/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4999 - val_o2_loss: 0.1979 - val_o1_acc: 0.9885 - val_o2_acc: 0.9873\n",
      "Epoch 250/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4989 - val_o2_loss: 0.1984 - val_o1_acc: 0.9886 - val_o2_acc: 0.9872\n",
      "Epoch 251/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4995 - val_o2_loss: 0.1981 - val_o1_acc: 0.9882 - val_o2_acc: 0.9877\n",
      "Epoch 252/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5006 - val_o2_loss: 0.1977 - val_o1_acc: 0.9885 - val_o2_acc: 0.9871\n",
      "Epoch 253/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5001 - val_o2_loss: 0.1979 - val_o1_acc: 0.9884 - val_o2_acc: 0.9878\n",
      "Epoch 254/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4987 - val_o2_loss: 0.1985 - val_o1_acc: 0.9885 - val_o2_acc: 0.9872\n",
      "Epoch 255/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4999 - val_o2_loss: 0.1980 - val_o1_acc: 0.9884 - val_o2_acc: 0.9874\n",
      "Epoch 256/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4986 - val_o2_loss: 0.1986 - val_o1_acc: 0.9883 - val_o2_acc: 0.9875\n",
      "Epoch 257/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.5004 - val_o2_loss: 0.1977 - val_o1_acc: 0.9883 - val_o2_acc: 0.9873\n",
      "Epoch 258/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9477 - val_o1_loss: 1.5002 - val_o2_loss: 0.1976 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 259/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4993 - val_o2_loss: 0.1982 - val_o1_acc: 0.9884 - val_o2_acc: 0.9873\n",
      "Epoch 260/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4998 - val_o2_loss: 0.1981 - val_o1_acc: 0.9882 - val_o2_acc: 0.9873\n",
      "Epoch 261/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5005 - val_o2_loss: 0.1976 - val_o1_acc: 0.9886 - val_o2_acc: 0.9874\n",
      "Epoch 262/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4988 - val_o2_loss: 0.1986 - val_o1_acc: 0.9883 - val_o2_acc: 0.9875\n",
      "Epoch 263/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4984 - val_o2_loss: 0.1989 - val_o1_acc: 0.9884 - val_o2_acc: 0.9874\n",
      "Epoch 264/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4995 - val_o2_loss: 0.1983 - val_o1_acc: 0.9885 - val_o2_acc: 0.9873\n",
      "Epoch 265/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4997 - val_o2_loss: 0.1981 - val_o1_acc: 0.9882 - val_o2_acc: 0.9876\n",
      "Epoch 266/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.4985 - val_o2_loss: 0.1985 - val_o1_acc: 0.9882 - val_o2_acc: 0.9875\n",
      "Epoch 267/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4992 - val_o2_loss: 0.1984 - val_o1_acc: 0.9884 - val_o2_acc: 0.9875\n",
      "Epoch 268/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4998 - val_o2_loss: 0.1980 - val_o1_acc: 0.9884 - val_o2_acc: 0.9876\n",
      "Epoch 269/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4987 - val_o2_loss: 0.1987 - val_o1_acc: 0.9884 - val_o2_acc: 0.9875\n",
      "Epoch 270/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9478 - val_o1_loss: 1.5002 - val_o2_loss: 0.1977 - val_o1_acc: 0.9886 - val_o2_acc: 0.9876\n",
      "Epoch 271/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.4989 - val_o2_loss: 0.1987 - val_o1_acc: 0.9884 - val_o2_acc: 0.9874\n",
      "Epoch 272/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5006 - val_o2_loss: 0.1977 - val_o1_acc: 0.9886 - val_o2_acc: 0.9878\n",
      "Epoch 273/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.5007 - val_o2_loss: 0.1976 - val_o1_acc: 0.9886 - val_o2_acc: 0.9875\n",
      "Epoch 274/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4929 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4985 - val_o2_loss: 0.1987 - val_o1_acc: 0.9885 - val_o2_acc: 0.9875\n",
      "Epoch 275/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4996 - val_o2_loss: 0.1983 - val_o1_acc: 0.9889 - val_o2_acc: 0.9875\n",
      "Epoch 276/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4998 - val_o2_loss: 0.1980 - val_o1_acc: 0.9887 - val_o2_acc: 0.9875\n",
      "Epoch 277/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4989 - val_o2_loss: 0.1985 - val_o1_acc: 0.9885 - val_o2_acc: 0.9875\n",
      "Epoch 278/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4994 - val_o2_loss: 0.1982 - val_o1_acc: 0.9886 - val_o2_acc: 0.9877\n",
      "Epoch 279/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9482 - val_o1_loss: 1.4991 - val_o2_loss: 0.1987 - val_o1_acc: 0.9885 - val_o2_acc: 0.9874\n",
      "Epoch 280/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4982 - val_o2_loss: 0.1990 - val_o1_acc: 0.9887 - val_o2_acc: 0.9875\n",
      "Epoch 281/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4996 - val_o2_loss: 0.1982 - val_o1_acc: 0.9884 - val_o2_acc: 0.9875\n",
      "Epoch 282/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4995 - val_o2_loss: 0.1983 - val_o1_acc: 0.9885 - val_o2_acc: 0.9878\n",
      "Epoch 283/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4998 - val_o2_loss: 0.1982 - val_o1_acc: 0.9884 - val_o2_acc: 0.9874\n",
      "Epoch 284/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4995 - val_o2_loss: 0.1983 - val_o1_acc: 0.9885 - val_o2_acc: 0.9876\n",
      "Epoch 285/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4991 - val_o2_loss: 0.1983 - val_o1_acc: 0.9885 - val_o2_acc: 0.9876\n",
      "Epoch 286/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4995 - val_o2_loss: 0.1983 - val_o1_acc: 0.9885 - val_o2_acc: 0.9876\n",
      "Epoch 287/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4999 - val_o2_loss: 0.1980 - val_o1_acc: 0.9886 - val_o2_acc: 0.9877\n",
      "Epoch 288/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4989 - val_o2_loss: 0.1984 - val_o1_acc: 0.9885 - val_o2_acc: 0.9877\n",
      "Epoch 289/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4998 - val_o2_loss: 0.1982 - val_o1_acc: 0.9887 - val_o2_acc: 0.9877\n",
      "Epoch 290/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4990 - val_o2_loss: 0.1985 - val_o1_acc: 0.9885 - val_o2_acc: 0.9874\n",
      "Epoch 291/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4990 - val_o2_loss: 0.1986 - val_o1_acc: 0.9886 - val_o2_acc: 0.9877\n",
      "Epoch 292/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4997 - val_o2_loss: 0.1981 - val_o1_acc: 0.9884 - val_o2_acc: 0.9874\n",
      "Epoch 293/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4997 - val_o2_loss: 0.1983 - val_o1_acc: 0.9884 - val_o2_acc: 0.9877\n",
      "Epoch 294/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4995 - val_o2_loss: 0.1984 - val_o1_acc: 0.9884 - val_o2_acc: 0.9873\n",
      "Epoch 295/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9479 - val_o1_loss: 1.4998 - val_o2_loss: 0.1980 - val_o1_acc: 0.9884 - val_o2_acc: 0.9875\n",
      "Epoch 296/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4995 - val_o2_loss: 0.1983 - val_o1_acc: 0.9884 - val_o2_acc: 0.9875\n",
      "Epoch 297/300\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4989 - val_o2_loss: 0.1986 - val_o1_acc: 0.9887 - val_o2_acc: 0.9877\n",
      "Epoch 298/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9005 - o1_loss: 1.4928 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4987 - val_o2_loss: 0.1987 - val_o1_acc: 0.9885 - val_o2_acc: 0.9876\n",
      "Epoch 299/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9004 - o1_loss: 1.4928 - o2_loss: 0.1541 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9480 - val_o1_loss: 1.4997 - val_o2_loss: 0.1982 - val_o1_acc: 0.9884 - val_o2_acc: 0.9875\n",
      "Epoch 300/300\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9004 - o1_loss: 1.4928 - o2_loss: 0.1540 - o1_acc: 0.9996 - o2_acc: 1.0000 - val_loss: 0.9481 - val_o1_loss: 1.4987 - val_o2_loss: 0.1988 - val_o1_acc: 0.9886 - val_o2_acc: 0.9877\n",
      "Test loss: 0.948113136959076\n",
      "Test accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "''' SOFT TEACHER IN CLASS '''\n",
    "reload(models)\n",
    "from keras import callbacks\n",
    "\n",
    "base_lr = 3e-3\n",
    "decay = 0.99\n",
    "optim = keras.optimizers.Adam(lr=base_lr)\n",
    "\n",
    "model['student_st'] = models.StudentModel(input_shape, num_classes, T=1, in_class=True, l2=0.1, b=1.8)\n",
    "model['student_st'].compile(loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
    "                          loss_weights=[0.5, 1.],\n",
    "                          optimizer=optim,\n",
    "                          metrics=['acc'])\n",
    "\n",
    "def schedule(epoch):\n",
    "    return base_lr * decay**(epoch)\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor='val_o2_loss', mode='min', verbose=0, patience=30)\n",
    "mc = callbacks.ModelCheckpoint('best_student_st.h5', monitor='val_o2_acc', mode='max', verbose=0, save_best_only=True)\n",
    "ls = callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "hist['student_st'] = model['student_st'].fit(x_train, [kd_gt['st_train'], y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=300,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, [kd_gt['st_test'], y_test]),\n",
    "          callbacks=[ls,],\n",
    "            )\n",
    "score['student_st'] = model['student_st'].evaluate(x_test, [kd_gt['st_test'], y_test], verbose=0)\n",
    "print('Test loss:', score['student_st'][0])\n",
    "print('Test accuracy:', score['student_st'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.948113136959076\n",
      "Test accuracy: 0.9877\n",
      "Test errors: 122\n"
     ]
    }
   ],
   "source": [
    "score['student_st'] = model['student_st'].evaluate(x_test, [kd_gt['st_test'], y_test], verbose=0)\n",
    "n_errors = np.int((1-score['student_st'][-1])*len(y_test))\n",
    "print('Test loss:', score['student_st'][0])\n",
    "print('Test accuracy:', score['student_st'][-1])\n",
    "print('Test errors:', n_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 3s 45us/step\n",
      "60000/60000 [==============================] - 3s 42us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.36443835, 0.6188533)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds['teacher'] = model['teacher'].T_model(T).predict(x_train, verbose=1, batch_size=batch_size)\n",
    "preds['soft_teacher'] = model['soft_teacher'].predict(x_train, verbose=1, batch_size=batch_size)\n",
    "\n",
    "import numpy as np\n",
    "np.linalg.norm(preds['teacher'], axis=-1).mean(), np.linalg.norm(preds['soft_teacher'], axis=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot constrained softmax probabilities generated by the model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ind = np.random.choice(len(preds['teacher']), 50)\n",
    "plt.plot(np.sort(preds['teacher'])[ind].T)\n",
    "plt.show()\n",
    "\n",
    "ind = np.random.choice(len(preds['soft_teacher']), 50)\n",
    "plt.plot(np.sort(preds['soft_teacher'])[ind].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 9), 4173),\n",
       " ((3, 5), 3223),\n",
       " ((7, 9), 3030),\n",
       " ((9, 4), 2460),\n",
       " ((5, 3), 2348),\n",
       " ((6, 5), 2334),\n",
       " ((1, 4), 2230),\n",
       " ((2, 3), 2195),\n",
       " ((0, 2), 2080),\n",
       " ((6, 4), 2067),\n",
       " ((1, 7), 1982),\n",
       " ((5, 9), 1881),\n",
       " ((0, 6), 1759),\n",
       " ((8, 5), 1475),\n",
       " ((9, 7), 1318),\n",
       " ((2, 8), 1316),\n",
       " ((1, 8), 1306),\n",
       " ((8, 9), 1202),\n",
       " ((8, 3), 1191),\n",
       " ((9, 8), 1178)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pairs = [(x[-1], x[-2]) for x in np.argsort(preds['teacher'])]\n",
    "counts = Counter(pairs)\n",
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pairs = [(x[0], x[1]) for x in np.argsort(preds_st)]\n",
    "counts = Counter(pairs)\n",
    "counts.most_common(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   3,   7,   9,  10,  66],\n",
       "       [  0,   2,   3,   4,   6, 100]], dtype=int32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = [[1, 10, 7, 9, 3, 66], [6, 4, 3, 2, 100, 0]]\n",
    "b = tf.sort(a,axis=-1,direction='ASCENDING',name=None)\n",
    "c = tf.keras.backend.eval(b)\n",
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
